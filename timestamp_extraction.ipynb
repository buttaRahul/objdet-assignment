{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2921813,"sourceType":"datasetVersion","datasetId":1421042},{"sourceId":8867389,"sourceType":"datasetVersion","datasetId":5336679},{"sourceId":8868992,"sourceType":"datasetVersion","datasetId":5337483}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installing Dependencies","metadata":{}},{"cell_type":"code","source":"pip install ultralytics==8.0.196","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:54:17.255759Z","iopub.execute_input":"2024-07-08T10:54:17.256637Z","iopub.status.idle":"2024-07-08T10:54:31.417198Z","shell.execute_reply.started":"2024-07-08T10:54:17.256603Z","shell.execute_reply":"2024-07-08T10:54:31.416045Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting ultralytics==8.0.196\n  Downloading ultralytics-8.0.196-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.0.196) (3.7.5)\nRequirement already satisfied: numpy>=1.22.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.0.196) (1.26.4)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.0.196) (4.10.0.82)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.0.196) (9.5.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.0.196) (6.0.1)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.0.196) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.0.196) (1.11.4)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.0.196) (2.1.2)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.0.196) (0.16.2)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.0.196) (4.66.4)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.0.196) (2.2.1)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.0.196) (0.12.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.0.196) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.0.196) (9.0.0)\nCollecting thop>=0.1.1 (from ultralytics==8.0.196)\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics==8.0.196) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics==8.0.196) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.0.196) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.0.196) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.0.196) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.0.196) (2024.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.0.196) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.0.196) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.0.196) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.0.196) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.0.196) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.0.196) (2024.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.0.196) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics==8.0.196) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics==8.0.196) (1.3.0)\nDownloading ultralytics-8.0.196-py3-none-any.whl (631 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m631.1/631.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nInstalling collected packages: thop, ultralytics\nSuccessfully installed thop-0.1.1.post2209072238 ultralytics-8.0.196\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install roboflow\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:54:31.419302Z","iopub.execute_input":"2024-07-08T10:54:31.419609Z","iopub.status.idle":"2024-07-08T10:54:47.590020Z","shell.execute_reply.started":"2024-07-08T10:54:31.419581Z","shell.execute_reply":"2024-07-08T10:54:47.588927Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting roboflow\n  Downloading roboflow-1.1.34-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from roboflow) (2024.2.2)\nCollecting chardet==4.0.0 (from roboflow)\n  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting idna==3.7 (from roboflow)\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: cycler in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.12.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.4.5)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from roboflow) (3.7.5)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.26.4)\nCollecting opencv-python-headless==4.10.0.84 (from roboflow)\n  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: Pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from roboflow) (9.5.0)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.9.0.post0)\nRequirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.0.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.32.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.16.0)\nRequirement already satisfied: urllib3>=1.26.6 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.26.18)\nRequirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.66.4)\nRequirement already satisfied: PyYAML>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from roboflow) (6.0.1)\nRequirement already satisfied: requests-toolbelt in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.10.1)\nCollecting python-magic (from roboflow)\n  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (1.2.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (4.47.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->roboflow) (3.3.2)\nDownloading roboflow-1.1.34-py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\nInstalling collected packages: python-magic, opencv-python-headless, idna, chardet, roboflow\n  Attempting uninstall: opencv-python-headless\n    Found existing installation: opencv-python-headless 4.10.0.82\n    Uninstalling opencv-python-headless-4.10.0.82:\n      Successfully uninstalled opencv-python-headless-4.10.0.82\n  Attempting uninstall: idna\n    Found existing installation: idna 3.6\n    Uninstalling idna-3.6:\n      Successfully uninstalled idna-3.6\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chardet-4.0.0 idna-3.7 opencv-python-headless-4.10.0.84 python-magic-0.4.27 roboflow-1.1.34\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install av","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:54:47.591642Z","iopub.execute_input":"2024-07-08T10:54:47.592046Z","iopub.status.idle":"2024-07-08T10:55:01.038884Z","shell.execute_reply.started":"2024-07-08T10:54:47.592010Z","shell.execute_reply":"2024-07-08T10:55:01.037790Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting av\n  Downloading av-12.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nDownloading av-12.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: av\nSuccessfully installed av-12.2.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import ultralytics\nimport torch\nfrom ultralytics import YOLO\nimport yaml\nimport os\nimport cv2\nimport time\nimport numpy as np\nimport av\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-08T10:55:01.041971Z","iopub.execute_input":"2024-07-08T10:55:01.042334Z","iopub.status.idle":"2024-07-08T10:55:06.051958Z","shell.execute_reply.started":"2024-07-08T10:55:01.042304Z","shell.execute_reply":"2024-07-08T10:55:06.051180Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Loading the Dataset","metadata":{}},{"cell_type":"code","source":"\n\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"MaXSsjPsJ1tghm68R4jI\")\nproject = rf.workspace(\"boycott-brands\").project(\"cola-vs-pepsi\")\nversion = project.version(1)\ndataset = version.download(\"yolov8\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:55:06.053066Z","iopub.execute_input":"2024-07-08T10:55:06.053569Z","iopub.status.idle":"2024-07-08T10:55:08.826398Z","shell.execute_reply.started":"2024-07-08T10:55:06.053529Z","shell.execute_reply":"2024-07-08T10:55:08.825523Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in cola-vs-pepsi--1 to yolov8:: 100%|██████████| 7837/7837 [00:00<00:00, 33689.38it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to cola-vs-pepsi--1 in yolov8:: 100%|██████████| 544/544 [00:00<00:00, 6683.48it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The dataset is in Yolov8 format","metadata":{}},{"cell_type":"code","source":"# Reading the yaml.data\n\ndef read_data_yaml(yaml_path):\n    with open(yaml_path, 'r') as file:\n        data = yaml.safe_load(file)\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:55:08.827645Z","iopub.execute_input":"2024-07-08T10:55:08.828046Z","iopub.status.idle":"2024-07-08T10:55:08.833280Z","shell.execute_reply.started":"2024-07-08T10:55:08.828013Z","shell.execute_reply":"2024-07-08T10:55:08.832171Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\nyaml_path = \"/kaggle/working/cola-vs-pepsi--1/data.yaml\"\ndata = read_data_yaml(yaml_path)\n    \nprint(data)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:55:08.834426Z","iopub.execute_input":"2024-07-08T10:55:08.834703Z","iopub.status.idle":"2024-07-08T10:55:08.847224Z","shell.execute_reply.started":"2024-07-08T10:55:08.834680Z","shell.execute_reply":"2024-07-08T10:55:08.846346Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{'names': ['Coca-Cola', 'Pepsi'], 'nc': 2, 'roboflow': {'license': 'CC BY 4.0', 'project': 'cola-vs-pepsi', 'url': 'https://universe.roboflow.com/boycott-brands/cola-vs-pepsi/dataset/1', 'version': 1, 'workspace': 'boycott-brands'}, 'test': 'test/images', 'train': 'train/images', 'val': 'valid/images'}\n","output_type":"stream"}]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:55:08.848204Z","iopub.execute_input":"2024-07-08T10:55:08.848498Z","iopub.status.idle":"2024-07-08T10:55:08.860265Z","shell.execute_reply.started":"2024-07-08T10:55:08.848474Z","shell.execute_reply":"2024-07-08T10:55:08.859413Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'names': ['Coca-Cola', 'Pepsi'],\n 'nc': 2,\n 'roboflow': {'license': 'CC BY 4.0',\n  'project': 'cola-vs-pepsi',\n  'url': 'https://universe.roboflow.com/boycott-brands/cola-vs-pepsi/dataset/1',\n  'version': 1,\n  'workspace': 'boycott-brands'},\n 'test': 'test/images',\n 'train': 'train/images',\n 'val': 'valid/images'}"},"metadata":{}}]},{"cell_type":"code","source":"data['train'] = '/kaggle/working/cola-vs-pepsi--1/train/images'\ndata['test'] = '/kaggle/working/cola-vs-pepsi--1/test/images'\ndata['valid'] = '/kaggle/working/cola-vs-pepsi--1/valid/images'","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:55:08.861460Z","iopub.execute_input":"2024-07-08T10:55:08.862109Z","iopub.status.idle":"2024-07-08T10:55:08.869385Z","shell.execute_reply.started":"2024-07-08T10:55:08.862054Z","shell.execute_reply":"2024-07-08T10:55:08.868532Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# if 'val' in data:\n#     del data['val']","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:55:08.872686Z","iopub.execute_input":"2024-07-08T10:55:08.872968Z","iopub.status.idle":"2024-07-08T10:55:08.880173Z","shell.execute_reply.started":"2024-07-08T10:55:08.872945Z","shell.execute_reply":"2024-07-08T10:55:08.879434Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:55:08.881144Z","iopub.execute_input":"2024-07-08T10:55:08.881424Z","iopub.status.idle":"2024-07-08T10:55:08.891909Z","shell.execute_reply.started":"2024-07-08T10:55:08.881402Z","shell.execute_reply":"2024-07-08T10:55:08.891034Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'names': ['Coca-Cola', 'Pepsi'],\n 'nc': 2,\n 'roboflow': {'license': 'CC BY 4.0',\n  'project': 'cola-vs-pepsi',\n  'url': 'https://universe.roboflow.com/boycott-brands/cola-vs-pepsi/dataset/1',\n  'version': 1,\n  'workspace': 'boycott-brands'},\n 'test': '/kaggle/working/cola-vs-pepsi--1/test/images',\n 'train': '/kaggle/working/cola-vs-pepsi--1/train/images',\n 'val': 'valid/images',\n 'valid': '/kaggle/working/cola-vs-pepsi--1/valid/images'}"},"metadata":{}}]},{"cell_type":"code","source":"with open(yaml_path, 'w') as file:\n    yaml.safe_dump(data, file)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:55:08.893086Z","iopub.execute_input":"2024-07-08T10:55:08.893940Z","iopub.status.idle":"2024-07-08T10:55:08.902791Z","shell.execute_reply.started":"2024-07-08T10:55:08.893908Z","shell.execute_reply":"2024-07-08T10:55:08.901963Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"data = read_data_yaml(yaml_path)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:55:08.903991Z","iopub.execute_input":"2024-07-08T10:55:08.904735Z","iopub.status.idle":"2024-07-08T10:55:08.916837Z","shell.execute_reply.started":"2024-07-08T10:55:08.904703Z","shell.execute_reply":"2024-07-08T10:55:08.915993Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'names': ['Coca-Cola', 'Pepsi'],\n 'nc': 2,\n 'roboflow': {'license': 'CC BY 4.0',\n  'project': 'cola-vs-pepsi',\n  'url': 'https://universe.roboflow.com/boycott-brands/cola-vs-pepsi/dataset/1',\n  'version': 1,\n  'workspace': 'boycott-brands'},\n 'test': '/kaggle/working/cola-vs-pepsi--1/test/images',\n 'train': '/kaggle/working/cola-vs-pepsi--1/train/images',\n 'val': 'valid/images',\n 'valid': '/kaggle/working/cola-vs-pepsi--1/valid/images'}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Loading Yolov8","metadata":{}},{"cell_type":"code","source":"model = YOLO(\"yolov8s.pt\")","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:55:08.917892Z","iopub.execute_input":"2024-07-08T10:55:08.918201Z","iopub.status.idle":"2024-07-08T10:55:09.739534Z","shell.execute_reply.started":"2024-07-08T10:55:08.918171Z","shell.execute_reply":"2024-07-08T10:55:09.738529Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to 'yolov8s.pt'...\n100%|██████████| 21.5M/21.5M [00:00<00:00, 182MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"\nos.environ['WANDB_MODE'] = 'disabled'","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:55:09.740822Z","iopub.execute_input":"2024-07-08T10:55:09.741123Z","iopub.status.idle":"2024-07-08T10:55:09.746252Z","shell.execute_reply.started":"2024-07-08T10:55:09.741096Z","shell.execute_reply":"2024-07-08T10:55:09.745494Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Training the Model","metadata":{}},{"cell_type":"code","source":"results = model.train(\n   data='/kaggle/working/cola-vs-pepsi--1/data.yaml',\n   imgsz=640,\n   epochs=60,\n   batch=8\n#    optimizer='Adam',  \n#    lr0=0.001         \n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T10:55:09.747200Z","iopub.execute_input":"2024-07-08T10:55:09.747870Z","iopub.status.idle":"2024-07-08T11:02:31.602830Z","shell.execute_reply.started":"2024-07-08T10:55:09.747830Z","shell.execute_reply":"2024-07-08T11:02:31.601558Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"New https://pypi.org/project/ultralytics/8.2.51 available 😃 Update with 'pip install -U ultralytics'\nUltralytics YOLOv8.0.196 🚀 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/kaggle/working/cola-vs-pepsi--1/data.yaml, epochs=60, patience=50, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n100%|██████████| 755k/755k [00:00<00:00, 25.7MB/s]\n2024-07-08 10:55:12,451\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n2024-07-08 10:55:13,679\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n2024-07-08 10:55:16.086969: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-08 10:55:16.087079: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-08 10:55:16.213194: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nOverriding model.yaml nc=80 with nc=2\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n 22        [15, 18, 21]  1   2116822  ultralytics.nn.modules.head.Detect           [2, [128, 256, 512]]          \nModel summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n\nTransferred 349/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\nDownloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n100%|██████████| 6.23M/6.23M [00:00<00:00, 119MB/s]\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/cola-vs-pepsi--1/train/labels... 245 images, 0 backgrounds, 0 corrupt: 100%|██████████| 245/245 [00:00<00:00, 1260.15it/s]\n\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/cola-vs-pepsi--1/train/labels.cache\nWARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 39, len(boxes) = 353. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\nos.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/cola-vs-pepsi--1/valid/labels... 15 images, 0 backgrounds, 0 corrupt: 100%|██████████| 15/15 [00:00<00:00, 1355.54it/s]\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/cola-vs-pepsi--1/valid/labels.cache\nPlotting labels to runs/detect/train/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/train\u001b[0m\nStarting training for 60 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/60      2.27G      1.287      2.604      1.528         20        640: 100%|██████████| 31/31 [00:07<00:00,  4.32it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n                   all         15         22       0.47      0.457      0.504      0.309\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/60      2.37G        1.2      1.706      1.462         11        640: 100%|██████████| 31/31 [00:05<00:00,  5.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.68it/s]\n                   all         15         22      0.291      0.452      0.224     0.0412\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/60      2.33G      1.288      1.631      1.493         17        640: 100%|██████████| 31/31 [00:05<00:00,  5.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n                   all         15         22     0.0785      0.281      0.102     0.0513\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/60      2.36G      1.339      1.561      1.523         14        640: 100%|██████████| 31/31 [00:05<00:00,  5.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.71it/s]\n                   all         15         22       0.75      0.176      0.404      0.193\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/60      2.33G      1.343      1.532      1.596          7        640: 100%|██████████| 31/31 [00:05<00:00,  5.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.07it/s]\n                   all         15         22      0.092      0.176     0.0773      0.045\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/60      2.45G       1.35      1.703      1.586         11        640: 100%|██████████| 31/31 [00:05<00:00,  5.84it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\n                   all         15         22     0.0504      0.205      0.032    0.00329\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/60      2.37G      1.311      1.642      1.555          7        640: 100%|██████████| 31/31 [00:05<00:00,  5.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.76it/s]\n                   all         15         22      0.815      0.529      0.622        0.4\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/60      2.46G      1.342      1.516      1.573          9        640: 100%|██████████| 31/31 [00:05<00:00,  5.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.68it/s]\n                   all         15         22      0.465      0.629      0.602      0.393\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/60      2.38G      1.218      1.354      1.452         14        640: 100%|██████████| 31/31 [00:05<00:00,  5.80it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.07it/s]\n                   all         15         22      0.636      0.529      0.519      0.319\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/60      2.46G      1.275      1.359      1.473         25        640: 100%|██████████| 31/31 [00:05<00:00,  5.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.83it/s]\n                   all         15         22      0.536      0.414      0.393      0.159\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      11/60      2.37G      1.192      1.292      1.426         15        640: 100%|██████████| 31/31 [00:05<00:00,  5.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.31it/s]\n                   all         15         22      0.465      0.448      0.422      0.194\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      12/60      2.41G       1.24      1.316      1.499         13        640: 100%|██████████| 31/31 [00:05<00:00,  5.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n                   all         15         22      0.634      0.614      0.589      0.287\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      13/60      2.33G      1.266      1.235      1.506         14        640: 100%|██████████| 31/31 [00:05<00:00,  5.80it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.97it/s]\n                   all         15         22       0.63      0.522      0.479      0.328\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      14/60      2.45G      1.164      1.238      1.434         18        640: 100%|██████████| 31/31 [00:05<00:00,  5.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.78it/s]\n                   all         15         22      0.898      0.524      0.636      0.371\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      15/60      2.42G      1.162      1.255      1.402         12        640: 100%|██████████| 31/31 [00:05<00:00,  5.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.11it/s]\n                   all         15         22      0.907      0.529      0.647       0.44\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      16/60      2.45G      1.164      1.227      1.415         15        640: 100%|██████████| 31/31 [00:05<00:00,  5.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n                   all         15         22      0.694      0.814      0.812      0.547\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      17/60      2.38G      1.138      1.147      1.399         15        640: 100%|██████████| 31/31 [00:05<00:00,  5.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.11it/s]\n                   all         15         22      0.842      0.622      0.663      0.461\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      18/60      2.42G      1.094      1.101      1.363         22        640: 100%|██████████| 31/31 [00:05<00:00,  5.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.77it/s]\n                   all         15         22      0.891      0.749      0.778      0.476\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      19/60       2.5G      1.105      1.094      1.334         12        640: 100%|██████████| 31/31 [00:05<00:00,  5.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n                   all         15         22      0.777      0.767      0.758      0.546\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      20/60      2.43G      1.123      1.099      1.344         19        640: 100%|██████████| 31/31 [00:05<00:00,  5.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]\n                   all         15         22        0.7      0.733      0.737       0.49\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      21/60      2.38G       1.11      1.093      1.333         14        640: 100%|██████████| 31/31 [00:05<00:00,  5.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.14it/s]\n                   all         15         22      0.778      0.819      0.847      0.579\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      22/60      2.38G      1.051     0.9976      1.314         16        640: 100%|██████████| 31/31 [00:05<00:00,  5.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]\n                   all         15         22      0.939      0.824      0.858      0.578\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      23/60      2.46G      1.092      1.038      1.329         17        640: 100%|██████████| 31/31 [00:05<00:00,  5.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.08it/s]\n                   all         15         22      0.571      0.578      0.663      0.485\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      24/60      2.37G      1.049     0.9805      1.299         19        640: 100%|██████████| 31/31 [00:05<00:00,  5.74it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]\n                   all         15         22      0.672      0.762      0.779      0.543\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      25/60      2.37G      1.012      1.019      1.301         15        640: 100%|██████████| 31/31 [00:05<00:00,  5.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.87it/s]\n                   all         15         22       0.93      0.807      0.847      0.555\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      26/60      2.49G      1.097      1.028      1.361         22        640: 100%|██████████| 31/31 [00:05<00:00,  5.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.78it/s]\n                   all         15         22      0.831      0.681      0.736      0.548\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      27/60      2.37G      1.024     0.9972      1.296         21        640: 100%|██████████| 31/31 [00:05<00:00,  5.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.15it/s]\n                   all         15         22       0.88      0.642      0.794      0.578\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      28/60      2.31G      1.032     0.9658      1.336         18        640: 100%|██████████| 31/31 [00:05<00:00,  5.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.77it/s]\n                   all         15         22      0.924      0.652      0.784      0.591\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      29/60      2.37G     0.9425     0.8633      1.249         30        640: 100%|██████████| 31/31 [00:05<00:00,  5.74it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.09it/s]\n                   all         15         22      0.878      0.635      0.748      0.502\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      30/60      2.46G     0.9976     0.8795      1.261         17        640: 100%|██████████| 31/31 [00:05<00:00,  5.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]\n                   all         15         22      0.851      0.629      0.797      0.532\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      31/60       2.5G       1.06     0.9258      1.335         18        640: 100%|██████████| 31/31 [00:05<00:00,  5.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.05it/s]\n                   all         15         22      0.879      0.732      0.803      0.594\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      32/60      2.38G     0.9441     0.8552      1.229         13        640: 100%|██████████| 31/31 [00:05<00:00,  5.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.16it/s]\n                   all         15         22      0.946      0.727      0.899      0.659\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      33/60      2.42G      0.991     0.8856      1.269         15        640: 100%|██████████| 31/31 [00:05<00:00,  5.72it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]\n                   all         15         22      0.798      0.791      0.848       0.61\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      34/60      2.32G     0.9482     0.8966      1.262          8        640: 100%|██████████| 31/31 [00:05<00:00,  5.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.96it/s]\n                   all         15         22      0.825      0.795      0.801      0.571\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      35/60      2.42G      0.974     0.8777      1.264         12        640: 100%|██████████| 31/31 [00:05<00:00,  5.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.05it/s]\n                   all         15         22      0.845      0.795       0.87      0.623\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      36/60      2.31G     0.9323     0.8085      1.231         17        640: 100%|██████████| 31/31 [00:05<00:00,  5.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.97it/s]\n                   all         15         22      0.883      0.862      0.948      0.668\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      37/60      2.38G     0.9305     0.8273       1.24         11        640: 100%|██████████| 31/31 [00:05<00:00,  5.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.06it/s]\n                   all         15         22      0.809      0.967      0.933      0.613\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      38/60      2.46G     0.9097     0.7636      1.199         10        640: 100%|██████████| 31/31 [00:05<00:00,  5.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.99it/s]\n                   all         15         22      0.842      0.862      0.923      0.625\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      39/60      2.45G     0.9937     0.8753      1.294         16        640: 100%|██████████| 31/31 [00:05<00:00,  5.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.90it/s]\n                   all         15         22      0.971       0.79      0.903       0.62\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      40/60      2.45G     0.8683     0.7745      1.166         17        640: 100%|██████████| 31/31 [00:05<00:00,  5.77it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.08it/s]\n                   all         15         22      0.847      0.852       0.84      0.604\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      41/60      2.38G     0.9167     0.7838      1.217         14        640: 100%|██████████| 31/31 [00:05<00:00,  5.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.02it/s]\n                   all         15         22      0.902      0.793      0.836      0.616\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      42/60      2.38G     0.9208     0.7816      1.223         12        640: 100%|██████████| 31/31 [00:05<00:00,  5.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.06it/s]\n                   all         15         22      0.986      0.666      0.781      0.598\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      43/60      2.38G     0.9025     0.7824      1.198         13        640: 100%|██████████| 31/31 [00:05<00:00,  5.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]\n                   all         15         22      0.869      0.697      0.826      0.602\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      44/60      2.41G     0.9013     0.7842       1.22         11        640: 100%|██████████| 31/31 [00:05<00:00,  5.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.05it/s]\n                   all         15         22      0.877        0.8      0.863      0.667\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      45/60      2.38G      0.878     0.7381      1.186         14        640: 100%|██████████| 31/31 [00:05<00:00,  5.77it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.93it/s]\n                   all         15         22      0.985      0.833      0.854      0.641\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      46/60      2.37G     0.8559     0.6955      1.162         15        640: 100%|██████████| 31/31 [00:05<00:00,  5.77it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.70it/s]\n                   all         15         22      0.924      0.788      0.831      0.621\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      47/60      2.45G     0.8582     0.7231      1.171         15        640: 100%|██████████| 31/31 [00:05<00:00,  5.74it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.50it/s]\n                   all         15         22      0.932        0.8      0.864       0.63\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      48/60      2.37G     0.8437     0.6836      1.185         19        640: 100%|██████████| 31/31 [00:05<00:00,  5.77it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]\n                   all         15         22      0.797      0.771      0.863      0.612\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      49/60       2.5G     0.8597     0.6872      1.172         19        640: 100%|██████████| 31/31 [00:05<00:00,  5.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.16it/s]\n                   all         15         22      0.772      0.765      0.823      0.624\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      50/60      2.32G        0.8     0.6682      1.143         15        640: 100%|██████████| 31/31 [00:05<00:00,  5.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.65it/s]\n                   all         15         22      0.837      0.683      0.805      0.586\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\nos.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\nos.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      51/60      2.37G     0.7807     0.6451      1.205          7        640: 100%|██████████| 31/31 [00:05<00:00,  5.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]\n                   all         15         22      0.947      0.704      0.817      0.611\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      52/60      2.36G     0.7193     0.5553       1.14         10        640: 100%|██████████| 31/31 [00:05<00:00,  5.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.66it/s]\n                   all         15         22       0.83      0.743       0.79      0.589\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      53/60      2.32G     0.7318     0.5688      1.152          9        640: 100%|██████████| 31/31 [00:05<00:00,  5.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.14it/s]\n                   all         15         22      0.819      0.793       0.82      0.612\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      54/60      2.32G     0.7223     0.5531      1.149          8        640: 100%|██████████| 31/31 [00:05<00:00,  5.82it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.93it/s]\n                   all         15         22      0.785      0.829      0.831      0.625\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      55/60       2.5G     0.7005     0.5403      1.132         13        640: 100%|██████████| 31/31 [00:05<00:00,  5.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.13it/s]\n                   all         15         22      0.813      0.829      0.838       0.61\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      56/60      2.32G     0.6694     0.5249      1.118          6        640: 100%|██████████| 31/31 [00:05<00:00,  5.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.09it/s]\n                   all         15         22      0.981      0.695      0.807      0.599\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      57/60      2.44G     0.7134     0.5518      1.169          5        640: 100%|██████████| 31/31 [00:05<00:00,  5.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.07it/s]\n                   all         15         22      0.926      0.695      0.791      0.613\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      58/60      2.37G     0.6722     0.5291      1.124          7        640: 100%|██████████| 31/31 [00:05<00:00,  5.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.14it/s]\n                   all         15         22      0.928      0.692      0.768      0.608\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      59/60      2.38G     0.6478     0.4842      1.117          8        640: 100%|██████████| 31/31 [00:05<00:00,  5.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.09it/s]\n                   all         15         22      0.984      0.695      0.781      0.605\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      60/60      2.31G      0.642     0.4795      1.107          9        640: 100%|██████████| 31/31 [00:05<00:00,  5.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n                   all         15         22      0.985      0.695      0.789      0.587\n\n60 epochs completed in 0.113 hours.\nOptimizer stripped from runs/detect/train/weights/last.pt, 22.5MB\nOptimizer stripped from runs/detect/train/weights/best.pt, 22.5MB\n\nValidating runs/detect/train/weights/best.pt...\nUltralytics YOLOv8.0.196 🚀 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nModel summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.56it/s]\n                   all         15         22      0.883      0.862      0.948      0.668\n             Coca-Cola         15          7        0.9      0.857      0.978      0.807\n                 Pepsi         15         15      0.866      0.867      0.918      0.529\nSpeed: 0.2ms preprocess, 4.8ms inference, 0.0ms loss, 1.3ms postprocess per image\nResults saved to \u001b[1mruns/detect/train\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"model = YOLO(\"/kaggle/working/runs/detect/train/weights/best.pt\")","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:02:31.605360Z","iopub.execute_input":"2024-07-08T11:02:31.605760Z","iopub.status.idle":"2024-07-08T11:02:31.687842Z","shell.execute_reply.started":"2024-07-08T11:02:31.605696Z","shell.execute_reply":"2024-07-08T11:02:31.686795Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:02:31.689602Z","iopub.execute_input":"2024-07-08T11:02:31.689977Z","iopub.status.idle":"2024-07-08T11:02:31.693983Z","shell.execute_reply.started":"2024-07-08T11:02:31.689943Z","shell.execute_reply":"2024-07-08T11:02:31.693102Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def detection_timestamps(video_path):\n    container = av.open(video_path)\n    \n    fps = container.streams.video[0].average_rate\n    frame_interval = int(fps)  \n\n    detection_dict = {}\n    frame_count = 0\n\n    for frame in container.decode(video=0):\n        frame_count += 1\n\n        if frame_count % frame_interval == 0:\n            current_time = round(frame.time, 2)  \n\n            img = frame.to_ndarray(format='bgr24')\n            results = model.predict(img)\n\n            frame_height, frame_width, _ = img.shape\n            frame_center = np.array([frame_width / 2, frame_height / 2])\n\n            for result in results:\n                if result.boxes is not None:\n                    for box in result.boxes:\n                        class_id = box.cls if box.cls is not None else -1 \n                        if class_id != -1:\n                            class_name = result.names[int(class_id)]  \n                            \n                            xyxy = box.xyxy[0].cpu().numpy()\n                            x_min, y_min, x_max, y_max = xyxy\n\n                            width = int(x_max - x_min)\n                            height = int(y_max - y_min)\n                            size = f\"{width}px X {height}px\" \n\n                            x_center = (x_min + x_max) / 2\n                            y_center = (y_min + y_max) / 2\n                            box_center = np.array([x_center, y_center])\n                            distance_from_center = round(np.linalg.norm(box_center - frame_center), 2) \n\n                            if class_name not in detection_dict:\n                                detection_dict[class_name] = []\n                            detection_dict[class_name].append({\n                                \"time_stamp\": current_time,\n                                \"size\": size,\n                                \"distance_from_center\": distance_from_center\n                            })\n\n    return detection_dict","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:02:31.695215Z","iopub.execute_input":"2024-07-08T11:02:31.695597Z","iopub.status.idle":"2024-07-08T11:02:31.708213Z","shell.execute_reply.started":"2024-07-08T11:02:31.695561Z","shell.execute_reply":"2024-07-08T11:02:31.707195Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\ndetections = detection_timestamps('/kaggle/input/video-pepsi-cola/videoplayback.mp4')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:02:31.709468Z","iopub.execute_input":"2024-07-08T11:02:31.710264Z","iopub.status.idle":"2024-07-08T11:04:33.434661Z","shell.execute_reply.started":"2024-07-08T11:02:31.710229Z","shell.execute_reply":"2024-07-08T11:04:33.433890Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"\n0: 384x640 2 Pepsis, 83.3ms\nSpeed: 2.8ms preprocess, 83.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 3 Pepsis, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 3 Pepsis, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 Pepsis, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.1ms\nSpeed: 2.9ms preprocess, 8.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.1ms\nSpeed: 3.0ms preprocess, 8.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.0ms\nSpeed: 2.1ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 1 Pepsi, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.1ms\nSpeed: 2.4ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.2ms\nSpeed: 2.3ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 8.1ms\nSpeed: 3.0ms preprocess, 8.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 Pepsis, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.1ms\nSpeed: 2.2ms preprocess, 8.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 10.3ms\nSpeed: 2.7ms preprocess, 10.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 Pepsis, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.5ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.2ms\nSpeed: 2.3ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.2ms\nSpeed: 2.9ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.2ms\nSpeed: 2.2ms preprocess, 8.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 9.3ms\nSpeed: 3.4ms preprocess, 9.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.5ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 8.1ms\nSpeed: 2.2ms preprocess, 8.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.8ms preprocess, 7.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.1ms\nSpeed: 2.3ms preprocess, 8.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 8.1ms\nSpeed: 3.1ms preprocess, 8.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.2ms\nSpeed: 2.8ms preprocess, 8.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.2ms\nSpeed: 2.2ms preprocess, 8.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 8.0ms\nSpeed: 2.2ms preprocess, 8.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.3ms\nSpeed: 2.3ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 Pepsis, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.4ms\nSpeed: 2.3ms preprocess, 8.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 8.2ms\nSpeed: 2.3ms preprocess, 8.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.5ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.7ms\nSpeed: 3.1ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.1ms\nSpeed: 3.1ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 3.0ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.4ms\nSpeed: 3.0ms preprocess, 8.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 8.2ms\nSpeed: 3.1ms preprocess, 8.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.1ms\nSpeed: 2.3ms preprocess, 8.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.1ms\nSpeed: 2.3ms preprocess, 8.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.5ms\nSpeed: 3.5ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.1ms\nSpeed: 2.9ms preprocess, 8.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.8ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.9ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.3ms\nSpeed: 2.3ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.2ms\nSpeed: 2.3ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.2ms\nSpeed: 2.4ms preprocess, 8.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.1ms\nSpeed: 2.3ms preprocess, 8.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.8ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.2ms\nSpeed: 3.4ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.1ms\nSpeed: 3.0ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.1ms\nSpeed: 2.4ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.0ms\nSpeed: 2.6ms preprocess, 9.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.3ms\nSpeed: 2.4ms preprocess, 8.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 2.6ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.1ms\nSpeed: 3.2ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.9ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 8.5ms\nSpeed: 2.9ms preprocess, 8.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Coca-Colas, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Coca-Colas, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Coca-Colas, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Coca-Colas, 8.2ms\nSpeed: 2.3ms preprocess, 8.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Coca-Colas, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Coca-Colas, 8.0ms\nSpeed: 2.2ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Coca-Colas, 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.1ms\nSpeed: 3.0ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.1ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 2.3ms preprocess, 8.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.1ms\nSpeed: 3.0ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.1ms\nSpeed: 2.9ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 8.4ms\nSpeed: 2.5ms preprocess, 8.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.6ms\nSpeed: 3.3ms preprocess, 8.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.6ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.5ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.7ms\nSpeed: 2.4ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.4ms\nSpeed: 2.5ms preprocess, 8.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.1ms\nSpeed: 2.1ms preprocess, 8.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 3.1ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.8ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.1ms\nSpeed: 2.9ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 2.6ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.1ms\nSpeed: 2.8ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.5ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.6ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.4ms\nSpeed: 2.6ms preprocess, 8.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.2ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.2ms\nSpeed: 3.0ms preprocess, 8.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.1ms preprocess, 7.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 9.1ms\nSpeed: 2.3ms preprocess, 9.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.2ms\nSpeed: 2.9ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.2ms\nSpeed: 2.2ms preprocess, 8.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.9ms\nSpeed: 2.1ms preprocess, 7.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.8ms\nSpeed: 2.6ms preprocess, 8.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 8.1ms\nSpeed: 3.0ms preprocess, 8.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.6ms\nSpeed: 3.1ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 8.0ms\nSpeed: 2.2ms preprocess, 8.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 9.4ms\nSpeed: 2.9ms preprocess, 9.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 13.0ms\nSpeed: 2.6ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.2ms\nSpeed: 2.8ms preprocess, 8.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.0ms\nSpeed: 2.2ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.3ms\nSpeed: 2.2ms preprocess, 8.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 3.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 9.4ms\nSpeed: 2.3ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.0ms\nSpeed: 2.1ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.0ms\nSpeed: 2.2ms preprocess, 8.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.2ms\nSpeed: 2.3ms preprocess, 8.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 9.2ms\nSpeed: 2.9ms preprocess, 9.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.4ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 8.0ms\nSpeed: 2.4ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 7.9ms\nSpeed: 3.3ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.1ms\nSpeed: 2.2ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.3ms\nSpeed: 2.2ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.2ms\nSpeed: 2.9ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 2.8ms preprocess, 8.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.1ms\nSpeed: 2.3ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 Pepsis, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 8.0ms\nSpeed: 2.1ms preprocess, 8.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.6ms\nSpeed: 3.1ms preprocess, 8.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.1ms\nSpeed: 2.3ms preprocess, 8.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.4ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.5ms preprocess, 7.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.9ms\nSpeed: 2.4ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 Pepsis, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.9ms\nSpeed: 2.5ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 Pepsis, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 8.0ms\nSpeed: 2.9ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 Pepsis, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 10.3ms\nSpeed: 3.0ms preprocess, 10.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.7ms\nSpeed: 2.4ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.2ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 10.5ms\nSpeed: 2.7ms preprocess, 10.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.2ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.3ms preprocess, 7.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 3.0ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Coca-Cola, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 8.2ms\nSpeed: 2.2ms preprocess, 8.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 Pepsis, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 10.1ms\nSpeed: 2.3ms preprocess, 10.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.6ms\nSpeed: 2.2ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.7ms\nSpeed: 3.0ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.7ms\nSpeed: 2.9ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 Pepsi, 7.9ms\nSpeed: 2.2ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.8ms\nSpeed: 2.1ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 8.4ms\nSpeed: 2.2ms preprocess, 8.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.6ms\nSpeed: 2.9ms preprocess, 7.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.8ms\nSpeed: 2.4ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.3ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.6ms\nSpeed: 3.0ms preprocess, 7.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 Pepsis, 8.0ms\nSpeed: 2.1ms preprocess, 8.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 Pepsis, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.6ms\nSpeed: 2.4ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.8ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.4ms\nSpeed: 2.3ms preprocess, 8.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 2.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.9ms\nSpeed: 2.9ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.8ms\nSpeed: 3.0ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\n# timestamps_json_object = json.dumps(detections,indent = 2)\n# print(detections)\ntimestamps_json_object = json.dumps(detections,indent = 4)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.435773Z","iopub.execute_input":"2024-07-08T11:04:33.436056Z","iopub.status.idle":"2024-07-08T11:04:33.446179Z","shell.execute_reply.started":"2024-07-08T11:04:33.436032Z","shell.execute_reply":"2024-07-08T11:04:33.445143Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print(timestamps_json_object)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.447455Z","iopub.execute_input":"2024-07-08T11:04:33.448083Z","iopub.status.idle":"2024-07-08T11:04:33.462430Z","shell.execute_reply.started":"2024-07-08T11:04:33.448050Z","shell.execute_reply":"2024-07-08T11:04:33.461542Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"{\n    \"Pepsi\": [\n        {\n            \"time_stamp\": 0.93,\n            \"size\": \"300px X 495px\",\n            \"distance_from_center\": 423.63\n        },\n        {\n            \"time_stamp\": 0.93,\n            \"size\": \"302px X 503px\",\n            \"distance_from_center\": 158.97\n        },\n        {\n            \"time_stamp\": 1.9,\n            \"size\": \"302px X 503px\",\n            \"distance_from_center\": 425.68\n        },\n        {\n            \"time_stamp\": 1.9,\n            \"size\": \"306px X 507px\",\n            \"distance_from_center\": 161.38\n        },\n        {\n            \"time_stamp\": 2.87,\n            \"size\": \"294px X 425px\",\n            \"distance_from_center\": 439.9\n        },\n        {\n            \"time_stamp\": 2.87,\n            \"size\": \"308px X 456px\",\n            \"distance_from_center\": 180.39\n        },\n        {\n            \"time_stamp\": 2.87,\n            \"size\": \"317px X 731px\",\n            \"distance_from_center\": 137.09\n        },\n        {\n            \"time_stamp\": 3.84,\n            \"size\": \"865px X 886px\",\n            \"distance_from_center\": 317.15\n        },\n        {\n            \"time_stamp\": 3.84,\n            \"size\": \"472px X 901px\",\n            \"distance_from_center\": 155.21\n        },\n        {\n            \"time_stamp\": 4.8,\n            \"size\": \"313px X 622px\",\n            \"distance_from_center\": 454.69\n        },\n        {\n            \"time_stamp\": 4.8,\n            \"size\": \"382px X 634px\",\n            \"distance_from_center\": 240.35\n        },\n        {\n            \"time_stamp\": 4.8,\n            \"size\": \"280px X 494px\",\n            \"distance_from_center\": 458.7\n        },\n        {\n            \"time_stamp\": 6.74,\n            \"size\": \"385px X 494px\",\n            \"distance_from_center\": 295.13\n        },\n        {\n            \"time_stamp\": 6.74,\n            \"size\": \"394px X 732px\",\n            \"distance_from_center\": 269.79\n        },\n        {\n            \"time_stamp\": 6.74,\n            \"size\": \"534px X 473px\",\n            \"distance_from_center\": 383.49\n        },\n        {\n            \"time_stamp\": 7.71,\n            \"size\": \"390px X 471px\",\n            \"distance_from_center\": 299.61\n        },\n        {\n            \"time_stamp\": 7.71,\n            \"size\": \"404px X 753px\",\n            \"distance_from_center\": 269.31\n        },\n        {\n            \"time_stamp\": 8.68,\n            \"size\": \"394px X 476px\",\n            \"distance_from_center\": 302.9\n        },\n        {\n            \"time_stamp\": 8.68,\n            \"size\": \"399px X 686px\",\n            \"distance_from_center\": 269.99\n        },\n        {\n            \"time_stamp\": 8.68,\n            \"size\": \"733px X 718px\",\n            \"distance_from_center\": 430.97\n        },\n        {\n            \"time_stamp\": 9.64,\n            \"size\": \"1728px X 777px\",\n            \"distance_from_center\": 178.94\n        },\n        {\n            \"time_stamp\": 10.61,\n            \"size\": \"962px X 470px\",\n            \"distance_from_center\": 437.6\n        },\n        {\n            \"time_stamp\": 11.58,\n            \"size\": \"991px X 458px\",\n            \"distance_from_center\": 453.39\n        },\n        {\n            \"time_stamp\": 12.55,\n            \"size\": \"403px X 787px\",\n            \"distance_from_center\": 305.98\n        },\n        {\n            \"time_stamp\": 12.55,\n            \"size\": \"624px X 780px\",\n            \"distance_from_center\": 417.36\n        },\n        {\n            \"time_stamp\": 13.51,\n            \"size\": \"373px X 455px\",\n            \"distance_from_center\": 329.0\n        },\n        {\n            \"time_stamp\": 13.51,\n            \"size\": \"365px X 658px\",\n            \"distance_from_center\": 235.94\n        },\n        {\n            \"time_stamp\": 13.51,\n            \"size\": \"893px X 578px\",\n            \"distance_from_center\": 445.68\n        },\n        {\n            \"time_stamp\": 13.51,\n            \"size\": \"597px X 685px\",\n            \"distance_from_center\": 294.86\n        },\n        {\n            \"time_stamp\": 15.45,\n            \"size\": \"384px X 681px\",\n            \"distance_from_center\": 142.3\n        },\n        {\n            \"time_stamp\": 16.42,\n            \"size\": \"550px X 767px\",\n            \"distance_from_center\": 200.34\n        },\n        {\n            \"time_stamp\": 17.38,\n            \"size\": \"324px X 555px\",\n            \"distance_from_center\": 134.75\n        },\n        {\n            \"time_stamp\": 18.35,\n            \"size\": \"322px X 592px\",\n            \"distance_from_center\": 165.53\n        },\n        {\n            \"time_stamp\": 19.32,\n            \"size\": \"547px X 469px\",\n            \"distance_from_center\": 133.83\n        },\n        {\n            \"time_stamp\": 19.32,\n            \"size\": \"1046px X 473px\",\n            \"distance_from_center\": 30.08\n        },\n        {\n            \"time_stamp\": 19.32,\n            \"size\": \"1116px X 473px\",\n            \"distance_from_center\": 234.83\n        },\n        {\n            \"time_stamp\": 21.25,\n            \"size\": \"461px X 298px\",\n            \"distance_from_center\": 511.96\n        },\n        {\n            \"time_stamp\": 22.22,\n            \"size\": \"1060px X 405px\",\n            \"distance_from_center\": 401.51\n        },\n        {\n            \"time_stamp\": 24.16,\n            \"size\": \"1012px X 435px\",\n            \"distance_from_center\": 258.98\n        },\n        {\n            \"time_stamp\": 25.13,\n            \"size\": \"986px X 297px\",\n            \"distance_from_center\": 420.35\n        },\n        {\n            \"time_stamp\": 25.13,\n            \"size\": \"723px X 303px\",\n            \"distance_from_center\": 506.85\n        },\n        {\n            \"time_stamp\": 29.0,\n            \"size\": \"720px X 853px\",\n            \"distance_from_center\": 261.17\n        },\n        {\n            \"time_stamp\": 29.96,\n            \"size\": \"734px X 825px\",\n            \"distance_from_center\": 300.7\n        },\n        {\n            \"time_stamp\": 30.93,\n            \"size\": \"717px X 860px\",\n            \"distance_from_center\": 250.62\n        },\n        {\n            \"time_stamp\": 31.9,\n            \"size\": \"731px X 858px\",\n            \"distance_from_center\": 288.96\n        },\n        {\n            \"time_stamp\": 32.87,\n            \"size\": \"679px X 798px\",\n            \"distance_from_center\": 355.55\n        },\n        {\n            \"time_stamp\": 34.8,\n            \"size\": \"870px X 678px\",\n            \"distance_from_center\": 213.78\n        },\n        {\n            \"time_stamp\": 34.8,\n            \"size\": \"1013px X 656px\",\n            \"distance_from_center\": 243.34\n        },\n        {\n            \"time_stamp\": 34.8,\n            \"size\": \"1445px X 652px\",\n            \"distance_from_center\": 199.63\n        },\n        {\n            \"time_stamp\": 35.77,\n            \"size\": \"401px X 697px\",\n            \"distance_from_center\": 144.1\n        },\n        {\n            \"time_stamp\": 36.74,\n            \"size\": \"612px X 715px\",\n            \"distance_from_center\": 183.6\n        },\n        {\n            \"time_stamp\": 37.7,\n            \"size\": \"704px X 743px\",\n            \"distance_from_center\": 168.51\n        },\n        {\n            \"time_stamp\": 37.7,\n            \"size\": \"1003px X 715px\",\n            \"distance_from_center\": 228.96\n        },\n        {\n            \"time_stamp\": 38.67,\n            \"size\": \"687px X 813px\",\n            \"distance_from_center\": 150.89\n        },\n        {\n            \"time_stamp\": 39.64,\n            \"size\": \"646px X 794px\",\n            \"distance_from_center\": 152.53\n        },\n        {\n            \"time_stamp\": 40.61,\n            \"size\": \"627px X 782px\",\n            \"distance_from_center\": 180.14\n        },\n        {\n            \"time_stamp\": 41.57,\n            \"size\": \"647px X 821px\",\n            \"distance_from_center\": 149.81\n        },\n        {\n            \"time_stamp\": 44.48,\n            \"size\": \"1840px X 1046px\",\n            \"distance_from_center\": 29.55\n        },\n        {\n            \"time_stamp\": 45.45,\n            \"size\": \"1658px X 618px\",\n            \"distance_from_center\": 221.22\n        },\n        {\n            \"time_stamp\": 46.41,\n            \"size\": \"697px X 656px\",\n            \"distance_from_center\": 60.22\n        },\n        {\n            \"time_stamp\": 47.38,\n            \"size\": \"386px X 676px\",\n            \"distance_from_center\": 35.79\n        },\n        {\n            \"time_stamp\": 48.35,\n            \"size\": \"347px X 904px\",\n            \"distance_from_center\": 89.77\n        },\n        {\n            \"time_stamp\": 49.32,\n            \"size\": \"325px X 496px\",\n            \"distance_from_center\": 44.93\n        },\n        {\n            \"time_stamp\": 56.09,\n            \"size\": \"285px X 611px\",\n            \"distance_from_center\": 146.55\n        },\n        {\n            \"time_stamp\": 56.09,\n            \"size\": \"433px X 616px\",\n            \"distance_from_center\": 184.02\n        },\n        {\n            \"time_stamp\": 57.06,\n            \"size\": \"287px X 611px\",\n            \"distance_from_center\": 146.61\n        },\n        {\n            \"time_stamp\": 57.06,\n            \"size\": \"422px X 618px\",\n            \"distance_from_center\": 179.3\n        },\n        {\n            \"time_stamp\": 60.93,\n            \"size\": \"603px X 756px\",\n            \"distance_from_center\": 90.01\n        },\n        {\n            \"time_stamp\": 61.9,\n            \"size\": \"1002px X 642px\",\n            \"distance_from_center\": 193.19\n        },\n        {\n            \"time_stamp\": 62.86,\n            \"size\": \"323px X 630px\",\n            \"distance_from_center\": 123.11\n        },\n        {\n            \"time_stamp\": 62.86,\n            \"size\": \"956px X 627px\",\n            \"distance_from_center\": 316.66\n        },\n        {\n            \"time_stamp\": 62.86,\n            \"size\": \"671px X 622px\",\n            \"distance_from_center\": 175.58\n        },\n        {\n            \"time_stamp\": 78.34,\n            \"size\": \"1034px X 740px\",\n            \"distance_from_center\": 407.65\n        },\n        {\n            \"time_stamp\": 78.34,\n            \"size\": \"328px X 730px\",\n            \"distance_from_center\": 93.31\n        },\n        {\n            \"time_stamp\": 79.31,\n            \"size\": \"327px X 966px\",\n            \"distance_from_center\": 50.69\n        },\n        {\n            \"time_stamp\": 79.31,\n            \"size\": \"925px X 976px\",\n            \"distance_from_center\": 313.95\n        },\n        {\n            \"time_stamp\": 80.28,\n            \"size\": \"319px X 977px\",\n            \"distance_from_center\": 53.09\n        },\n        {\n            \"time_stamp\": 80.28,\n            \"size\": \"616px X 977px\",\n            \"distance_from_center\": 161.34\n        },\n        {\n            \"time_stamp\": 81.25,\n            \"size\": \"319px X 769px\",\n            \"distance_from_center\": 65.48\n        },\n        {\n            \"time_stamp\": 82.22,\n            \"size\": \"318px X 921px\",\n            \"distance_from_center\": 19.53\n        },\n        {\n            \"time_stamp\": 83.18,\n            \"size\": \"329px X 949px\",\n            \"distance_from_center\": 25.5\n        },\n        {\n            \"time_stamp\": 84.15,\n            \"size\": \"342px X 937px\",\n            \"distance_from_center\": 12.93\n        },\n        {\n            \"time_stamp\": 85.12,\n            \"size\": \"329px X 884px\",\n            \"distance_from_center\": 35.71\n        },\n        {\n            \"time_stamp\": 86.09,\n            \"size\": \"362px X 957px\",\n            \"distance_from_center\": 16.44\n        },\n        {\n            \"time_stamp\": 87.05,\n            \"size\": \"335px X 749px\",\n            \"distance_from_center\": 91.83\n        },\n        {\n            \"time_stamp\": 88.02,\n            \"size\": \"349px X 725px\",\n            \"distance_from_center\": 109.22\n        },\n        {\n            \"time_stamp\": 88.02,\n            \"size\": \"1143px X 763px\",\n            \"distance_from_center\": 347.64\n        },\n        {\n            \"time_stamp\": 88.99,\n            \"size\": \"512px X 933px\",\n            \"distance_from_center\": 73.94\n        },\n        {\n            \"time_stamp\": 88.99,\n            \"size\": \"1307px X 941px\",\n            \"distance_from_center\": 295.27\n        },\n        {\n            \"time_stamp\": 88.99,\n            \"size\": \"833px X 946px\",\n            \"distance_from_center\": 229.81\n        },\n        {\n            \"time_stamp\": 88.99,\n            \"size\": \"346px X 924px\",\n            \"distance_from_center\": 8.13\n        },\n        {\n            \"time_stamp\": 89.96,\n            \"size\": \"374px X 928px\",\n            \"distance_from_center\": 21.2\n        },\n        {\n            \"time_stamp\": 89.96,\n            \"size\": \"1122px X 943px\",\n            \"distance_from_center\": 328.8\n        },\n        {\n            \"time_stamp\": 90.92,\n            \"size\": \"352px X 944px\",\n            \"distance_from_center\": 10.06\n        },\n        {\n            \"time_stamp\": 90.92,\n            \"size\": \"1140px X 954px\",\n            \"distance_from_center\": 350.89\n        },\n        {\n            \"time_stamp\": 91.89,\n            \"size\": \"354px X 953px\",\n            \"distance_from_center\": 18.05\n        },\n        {\n            \"time_stamp\": 91.89,\n            \"size\": \"1103px X 963px\",\n            \"distance_from_center\": 348.08\n        },\n        {\n            \"time_stamp\": 92.86,\n            \"size\": \"257px X 403px\",\n            \"distance_from_center\": 136.73\n        },\n        {\n            \"time_stamp\": 92.86,\n            \"size\": \"593px X 596px\",\n            \"distance_from_center\": 217.38\n        },\n        {\n            \"time_stamp\": 92.86,\n            \"size\": \"365px X 609px\",\n            \"distance_from_center\": 214.79\n        },\n        {\n            \"time_stamp\": 93.83,\n            \"size\": \"547px X 650px\",\n            \"distance_from_center\": 215.23\n        },\n        {\n            \"time_stamp\": 94.79,\n            \"size\": \"1814px X 854px\",\n            \"distance_from_center\": 119.01\n        },\n        {\n            \"time_stamp\": 95.76,\n            \"size\": \"1772px X 922px\",\n            \"distance_from_center\": 90.82\n        },\n        {\n            \"time_stamp\": 96.73,\n            \"size\": \"1425px X 982px\",\n            \"distance_from_center\": 247.48\n        },\n        {\n            \"time_stamp\": 97.7,\n            \"size\": \"1177px X 941px\",\n            \"distance_from_center\": 361.35\n        },\n        {\n            \"time_stamp\": 97.7,\n            \"size\": \"872px X 936px\",\n            \"distance_from_center\": 85.01\n        },\n        {\n            \"time_stamp\": 98.67,\n            \"size\": \"665px X 656px\",\n            \"distance_from_center\": 207.85\n        },\n        {\n            \"time_stamp\": 98.67,\n            \"size\": \"1225px X 623px\",\n            \"distance_from_center\": 385.02\n        },\n        {\n            \"time_stamp\": 98.67,\n            \"size\": \"1266px X 897px\",\n            \"distance_from_center\": 329.57\n        },\n        {\n            \"time_stamp\": 98.67,\n            \"size\": \"822px X 796px\",\n            \"distance_from_center\": 195.77\n        },\n        {\n            \"time_stamp\": 101.57,\n            \"size\": \"697px X 731px\",\n            \"distance_from_center\": 235.87\n        },\n        {\n            \"time_stamp\": 103.5,\n            \"size\": \"490px X 799px\",\n            \"distance_from_center\": 108.23\n        },\n        {\n            \"time_stamp\": 104.47,\n            \"size\": \"426px X 821px\",\n            \"distance_from_center\": 135.9\n        },\n        {\n            \"time_stamp\": 105.44,\n            \"size\": \"401px X 641px\",\n            \"distance_from_center\": 222.21\n        },\n        {\n            \"time_stamp\": 106.41,\n            \"size\": \"394px X 477px\",\n            \"distance_from_center\": 98.26\n        },\n        {\n            \"time_stamp\": 107.37,\n            \"size\": \"557px X 847px\",\n            \"distance_from_center\": 120.81\n        },\n        {\n            \"time_stamp\": 109.31,\n            \"size\": \"1473px X 895px\",\n            \"distance_from_center\": 86.55\n        },\n        {\n            \"time_stamp\": 111.24,\n            \"size\": \"469px X 961px\",\n            \"distance_from_center\": 74.48\n        },\n        {\n            \"time_stamp\": 113.18,\n            \"size\": \"420px X 885px\",\n            \"distance_from_center\": 67.74\n        },\n        {\n            \"time_stamp\": 113.18,\n            \"size\": \"1020px X 917px\",\n            \"distance_from_center\": 229.28\n        },\n        {\n            \"time_stamp\": 114.15,\n            \"size\": \"358px X 526px\",\n            \"distance_from_center\": 183.35\n        },\n        {\n            \"time_stamp\": 115.11,\n            \"size\": \"357px X 521px\",\n            \"distance_from_center\": 187.73\n        },\n        {\n            \"time_stamp\": 115.11,\n            \"size\": \"379px X 703px\",\n            \"distance_from_center\": 98.85\n        },\n        {\n            \"time_stamp\": 116.08,\n            \"size\": \"926px X 427px\",\n            \"distance_from_center\": 102.18\n        },\n        {\n            \"time_stamp\": 117.05,\n            \"size\": \"946px X 418px\",\n            \"distance_from_center\": 107.9\n        },\n        {\n            \"time_stamp\": 118.02,\n            \"size\": \"960px X 359px\",\n            \"distance_from_center\": 179.75\n        },\n        {\n            \"time_stamp\": 118.99,\n            \"size\": \"747px X 411px\",\n            \"distance_from_center\": 84.7\n        },\n        {\n            \"time_stamp\": 119.95,\n            \"size\": \"937px X 505px\",\n            \"distance_from_center\": 113.68\n        },\n        {\n            \"time_stamp\": 120.92,\n            \"size\": \"944px X 475px\",\n            \"distance_from_center\": 145.62\n        },\n        {\n            \"time_stamp\": 120.92,\n            \"size\": \"601px X 462px\",\n            \"distance_from_center\": 220.97\n        },\n        {\n            \"time_stamp\": 121.89,\n            \"size\": \"816px X 846px\",\n            \"distance_from_center\": 141.78\n        },\n        {\n            \"time_stamp\": 122.86,\n            \"size\": \"1247px X 793px\",\n            \"distance_from_center\": 331.55\n        },\n        {\n            \"time_stamp\": 124.79,\n            \"size\": \"1097px X 723px\",\n            \"distance_from_center\": 410.32\n        },\n        {\n            \"time_stamp\": 125.76,\n            \"size\": \"660px X 518px\",\n            \"distance_from_center\": 160.63\n        },\n        {\n            \"time_stamp\": 126.73,\n            \"size\": \"445px X 802px\",\n            \"distance_from_center\": 188.71\n        },\n        {\n            \"time_stamp\": 127.69,\n            \"size\": \"631px X 650px\",\n            \"distance_from_center\": 105.4\n        },\n        {\n            \"time_stamp\": 128.66,\n            \"size\": \"1212px X 966px\",\n            \"distance_from_center\": 339.98\n        },\n        {\n            \"time_stamp\": 129.63,\n            \"size\": \"742px X 904px\",\n            \"distance_from_center\": 56.7\n        },\n        {\n            \"time_stamp\": 129.63,\n            \"size\": \"1138px X 914px\",\n            \"distance_from_center\": 210.32\n        },\n        {\n            \"time_stamp\": 129.63,\n            \"size\": \"864px X 508px\",\n            \"distance_from_center\": 578.63\n        },\n        {\n            \"time_stamp\": 130.6,\n            \"size\": \"1170px X 939px\",\n            \"distance_from_center\": 324.67\n        },\n        {\n            \"time_stamp\": 130.6,\n            \"size\": \"555px X 396px\",\n            \"distance_from_center\": 762.87\n        },\n        {\n            \"time_stamp\": 130.6,\n            \"size\": \"675px X 806px\",\n            \"distance_from_center\": 344.94\n        },\n        {\n            \"time_stamp\": 131.56,\n            \"size\": \"1228px X 816px\",\n            \"distance_from_center\": 321.48\n        },\n        {\n            \"time_stamp\": 131.56,\n            \"size\": \"687px X 814px\",\n            \"distance_from_center\": 72.29\n        },\n        {\n            \"time_stamp\": 131.56,\n            \"size\": \"772px X 532px\",\n            \"distance_from_center\": 291.63\n        },\n        {\n            \"time_stamp\": 132.53,\n            \"size\": \"1021px X 889px\",\n            \"distance_from_center\": 252.93\n        },\n        {\n            \"time_stamp\": 134.47,\n            \"size\": \"690px X 909px\",\n            \"distance_from_center\": 108.18\n        },\n        {\n            \"time_stamp\": 135.44,\n            \"size\": \"593px X 969px\",\n            \"distance_from_center\": 97.53\n        },\n        {\n            \"time_stamp\": 135.44,\n            \"size\": \"553px X 650px\",\n            \"distance_from_center\": 137.31\n        },\n        {\n            \"time_stamp\": 135.44,\n            \"size\": \"988px X 918px\",\n            \"distance_from_center\": 237.55\n        },\n        {\n            \"time_stamp\": 135.44,\n            \"size\": \"555px X 367px\",\n            \"distance_from_center\": 222.19\n        },\n        {\n            \"time_stamp\": 135.44,\n            \"size\": \"1205px X 643px\",\n            \"distance_from_center\": 394.79\n        },\n        {\n            \"time_stamp\": 136.4,\n            \"size\": \"614px X 802px\",\n            \"distance_from_center\": 160.56\n        },\n        {\n            \"time_stamp\": 137.37,\n            \"size\": \"526px X 712px\",\n            \"distance_from_center\": 165.64\n        },\n        {\n            \"time_stamp\": 138.34,\n            \"size\": \"565px X 692px\",\n            \"distance_from_center\": 184.61\n        },\n        {\n            \"time_stamp\": 139.31,\n            \"size\": \"578px X 724px\",\n            \"distance_from_center\": 204.72\n        },\n        {\n            \"time_stamp\": 140.27,\n            \"size\": \"569px X 695px\",\n            \"distance_from_center\": 208.66\n        },\n        {\n            \"time_stamp\": 141.24,\n            \"size\": \"529px X 694px\",\n            \"distance_from_center\": 188.56\n        },\n        {\n            \"time_stamp\": 142.21,\n            \"size\": \"534px X 794px\",\n            \"distance_from_center\": 111.11\n        },\n        {\n            \"time_stamp\": 142.21,\n            \"size\": \"805px X 795px\",\n            \"distance_from_center\": 206.27\n        },\n        {\n            \"time_stamp\": 143.18,\n            \"size\": \"476px X 821px\",\n            \"distance_from_center\": 142.17\n        },\n        {\n            \"time_stamp\": 144.14,\n            \"size\": \"459px X 747px\",\n            \"distance_from_center\": 13.15\n        },\n        {\n            \"time_stamp\": 147.05,\n            \"size\": \"1241px X 474px\",\n            \"distance_from_center\": 341.97\n        },\n        {\n            \"time_stamp\": 150.92,\n            \"size\": \"373px X 570px\",\n            \"distance_from_center\": 248.65\n        },\n        {\n            \"time_stamp\": 150.92,\n            \"size\": \"865px X 558px\",\n            \"distance_from_center\": 424.62\n        },\n        {\n            \"time_stamp\": 151.89,\n            \"size\": \"847px X 597px\",\n            \"distance_from_center\": 407.51\n        },\n        {\n            \"time_stamp\": 151.89,\n            \"size\": \"498px X 586px\",\n            \"distance_from_center\": 283.68\n        },\n        {\n            \"time_stamp\": 153.82,\n            \"size\": \"573px X 428px\",\n            \"distance_from_center\": 322.04\n        },\n        {\n            \"time_stamp\": 153.82,\n            \"size\": \"1356px X 426px\",\n            \"distance_from_center\": 379.45\n        },\n        {\n            \"time_stamp\": 155.76,\n            \"size\": \"457px X 751px\",\n            \"distance_from_center\": 107.47\n        },\n        {\n            \"time_stamp\": 156.72,\n            \"size\": \"586px X 380px\",\n            \"distance_from_center\": 242.13\n        },\n        {\n            \"time_stamp\": 156.72,\n            \"size\": \"858px X 403px\",\n            \"distance_from_center\": 328.37\n        },\n        {\n            \"time_stamp\": 156.72,\n            \"size\": \"514px X 452px\",\n            \"distance_from_center\": 168.25\n        },\n        {\n            \"time_stamp\": 157.69,\n            \"size\": \"566px X 331px\",\n            \"distance_from_center\": 263.45\n        },\n        {\n            \"time_stamp\": 157.69,\n            \"size\": \"555px X 445px\",\n            \"distance_from_center\": 209.56\n        },\n        {\n            \"time_stamp\": 164.46,\n            \"size\": \"386px X 571px\",\n            \"distance_from_center\": 153.76\n        },\n        {\n            \"time_stamp\": 167.37,\n            \"size\": \"489px X 646px\",\n            \"distance_from_center\": 173.2\n        },\n        {\n            \"time_stamp\": 171.24,\n            \"size\": \"647px X 505px\",\n            \"distance_from_center\": 217.88\n        },\n        {\n            \"time_stamp\": 174.14,\n            \"size\": \"907px X 722px\",\n            \"distance_from_center\": 488.03\n        },\n        {\n            \"time_stamp\": 174.14,\n            \"size\": \"445px X 713px\",\n            \"distance_from_center\": 271.14\n        },\n        {\n            \"time_stamp\": 177.04,\n            \"size\": \"372px X 644px\",\n            \"distance_from_center\": 105.2\n        },\n        {\n            \"time_stamp\": 178.01,\n            \"size\": \"371px X 743px\",\n            \"distance_from_center\": 136.05\n        },\n        {\n            \"time_stamp\": 178.98,\n            \"size\": \"353px X 763px\",\n            \"distance_from_center\": 162.0\n        },\n        {\n            \"time_stamp\": 179.95,\n            \"size\": \"376px X 753px\",\n            \"distance_from_center\": 166.8\n        },\n        {\n            \"time_stamp\": 180.91,\n            \"size\": \"589px X 553px\",\n            \"distance_from_center\": 329.21\n        },\n        {\n            \"time_stamp\": 181.88,\n            \"size\": \"619px X 553px\",\n            \"distance_from_center\": 324.8\n        },\n        {\n            \"time_stamp\": 182.85,\n            \"size\": \"583px X 549px\",\n            \"distance_from_center\": 336.28\n        },\n        {\n            \"time_stamp\": 183.82,\n            \"size\": \"488px X 537px\",\n            \"distance_from_center\": 367.52\n        },\n        {\n            \"time_stamp\": 184.78,\n            \"size\": \"614px X 539px\",\n            \"distance_from_center\": 329.58\n        },\n        {\n            \"time_stamp\": 185.75,\n            \"size\": \"630px X 545px\",\n            \"distance_from_center\": 324.65\n        },\n        {\n            \"time_stamp\": 186.72,\n            \"size\": \"651px X 543px\",\n            \"distance_from_center\": 343.42\n        },\n        {\n            \"time_stamp\": 187.69,\n            \"size\": \"667px X 524px\",\n            \"distance_from_center\": 369.28\n        },\n        {\n            \"time_stamp\": 188.66,\n            \"size\": \"1745px X 838px\",\n            \"distance_from_center\": 81.78\n        },\n        {\n            \"time_stamp\": 189.62,\n            \"size\": \"795px X 887px\",\n            \"distance_from_center\": 556.04\n        },\n        {\n            \"time_stamp\": 195.43,\n            \"size\": \"602px X 741px\",\n            \"distance_from_center\": 194.93\n        },\n        {\n            \"time_stamp\": 196.4,\n            \"size\": \"521px X 729px\",\n            \"distance_from_center\": 139.33\n        },\n        {\n            \"time_stamp\": 197.36,\n            \"size\": \"605px X 443px\",\n            \"distance_from_center\": 470.15\n        },\n        {\n            \"time_stamp\": 197.36,\n            \"size\": \"568px X 393px\",\n            \"distance_from_center\": 368.84\n        },\n        {\n            \"time_stamp\": 198.33,\n            \"size\": \"1007px X 261px\",\n            \"distance_from_center\": 610.09\n        },\n        {\n            \"time_stamp\": 199.3,\n            \"size\": \"1129px X 762px\",\n            \"distance_from_center\": 423.67\n        },\n        {\n            \"time_stamp\": 200.27,\n            \"size\": \"1076px X 651px\",\n            \"distance_from_center\": 472.98\n        },\n        {\n            \"time_stamp\": 201.23,\n            \"size\": \"1075px X 743px\",\n            \"distance_from_center\": 454.63\n        },\n        {\n            \"time_stamp\": 202.2,\n            \"size\": \"1049px X 622px\",\n            \"distance_from_center\": 491.87\n        },\n        {\n            \"time_stamp\": 203.17,\n            \"size\": \"1018px X 561px\",\n            \"distance_from_center\": 514.94\n        },\n        {\n            \"time_stamp\": 210.91,\n            \"size\": \"1096px X 664px\",\n            \"distance_from_center\": 455.23\n        },\n        {\n            \"time_stamp\": 245.75,\n            \"size\": \"520px X 740px\",\n            \"distance_from_center\": 170.72\n        },\n        {\n            \"time_stamp\": 269.94,\n            \"size\": \"394px X 734px\",\n            \"distance_from_center\": 246.22\n        },\n        {\n            \"time_stamp\": 270.9,\n            \"size\": \"416px X 768px\",\n            \"distance_from_center\": 271.29\n        },\n        {\n            \"time_stamp\": 271.87,\n            \"size\": \"433px X 724px\",\n            \"distance_from_center\": 284.08\n        },\n        {\n            \"time_stamp\": 272.84,\n            \"size\": \"911px X 727px\",\n            \"distance_from_center\": 303.72\n        },\n        {\n            \"time_stamp\": 273.81,\n            \"size\": \"396px X 598px\",\n            \"distance_from_center\": 242.24\n        },\n        {\n            \"time_stamp\": 274.77,\n            \"size\": \"468px X 706px\",\n            \"distance_from_center\": 187.32\n        },\n        {\n            \"time_stamp\": 275.74,\n            \"size\": \"428px X 545px\",\n            \"distance_from_center\": 267.47\n        },\n        {\n            \"time_stamp\": 277.68,\n            \"size\": \"383px X 569px\",\n            \"distance_from_center\": 256.44\n        },\n        {\n            \"time_stamp\": 278.65,\n            \"size\": \"421px X 574px\",\n            \"distance_from_center\": 255.19\n        },\n        {\n            \"time_stamp\": 279.61,\n            \"size\": \"674px X 620px\",\n            \"distance_from_center\": 218.76\n        },\n        {\n            \"time_stamp\": 281.55,\n            \"size\": \"402px X 615px\",\n            \"distance_from_center\": 333.92\n        },\n        {\n            \"time_stamp\": 310.58,\n            \"size\": \"1277px X 615px\",\n            \"distance_from_center\": 366.79\n        },\n        {\n            \"time_stamp\": 312.51,\n            \"size\": \"1328px X 571px\",\n            \"distance_from_center\": 364.68\n        },\n        {\n            \"time_stamp\": 313.48,\n            \"size\": \"1674px X 623px\",\n            \"distance_from_center\": 246.3\n        },\n        {\n            \"time_stamp\": 314.45,\n            \"size\": \"1619px X 623px\",\n            \"distance_from_center\": 253.82\n        },\n        {\n            \"time_stamp\": 316.38,\n            \"size\": \"436px X 812px\",\n            \"distance_from_center\": 131.75\n        },\n        {\n            \"time_stamp\": 317.35,\n            \"size\": \"437px X 805px\",\n            \"distance_from_center\": 119.45\n        },\n        {\n            \"time_stamp\": 317.35,\n            \"size\": \"659px X 809px\",\n            \"distance_from_center\": 152.8\n        },\n        {\n            \"time_stamp\": 318.32,\n            \"size\": \"1114px X 843px\",\n            \"distance_from_center\": 346.65\n        },\n        {\n            \"time_stamp\": 320.25,\n            \"size\": \"988px X 461px\",\n            \"distance_from_center\": 331.94\n        },\n        {\n            \"time_stamp\": 321.22,\n            \"size\": \"926px X 539px\",\n            \"distance_from_center\": 228.44\n        },\n        {\n            \"time_stamp\": 329.93,\n            \"size\": \"1190px X 566px\",\n            \"distance_from_center\": 346.36\n        },\n        {\n            \"time_stamp\": 329.93,\n            \"size\": \"923px X 516px\",\n            \"distance_from_center\": 273.16\n        },\n        {\n            \"time_stamp\": 330.9,\n            \"size\": \"678px X 728px\",\n            \"distance_from_center\": 269.53\n        },\n        {\n            \"time_stamp\": 331.86,\n            \"size\": \"453px X 862px\",\n            \"distance_from_center\": 55.89\n        },\n        {\n            \"time_stamp\": 332.83,\n            \"size\": \"381px X 882px\",\n            \"distance_from_center\": 62.11\n        },\n        {\n            \"time_stamp\": 333.8,\n            \"size\": \"444px X 821px\",\n            \"distance_from_center\": 79.39\n        },\n        {\n            \"time_stamp\": 337.67,\n            \"size\": \"482px X 854px\",\n            \"distance_from_center\": 63.84\n        },\n        {\n            \"time_stamp\": 338.64,\n            \"size\": \"451px X 752px\",\n            \"distance_from_center\": 54.17\n        },\n        {\n            \"time_stamp\": 338.64,\n            \"size\": \"491px X 551px\",\n            \"distance_from_center\": 127.95\n        },\n        {\n            \"time_stamp\": 339.61,\n            \"size\": \"438px X 734px\",\n            \"distance_from_center\": 44.22\n        },\n        {\n            \"time_stamp\": 363.8,\n            \"size\": \"773px X 603px\",\n            \"distance_from_center\": 270.08\n        },\n        {\n            \"time_stamp\": 376.38,\n            \"size\": \"953px X 699px\",\n            \"distance_from_center\": 519.11\n        },\n        {\n            \"time_stamp\": 378.31,\n            \"size\": \"1122px X 422px\",\n            \"distance_from_center\": 516.57\n        },\n        {\n            \"time_stamp\": 380.25,\n            \"size\": \"327px X 1075px\",\n            \"distance_from_center\": 796.15\n        },\n        {\n            \"time_stamp\": 383.15,\n            \"size\": \"201px X 1080px\",\n            \"distance_from_center\": 859.06\n        },\n        {\n            \"time_stamp\": 384.12,\n            \"size\": \"215px X 1080px\",\n            \"distance_from_center\": 851.79\n        },\n        {\n            \"time_stamp\": 385.08,\n            \"size\": \"1816px X 1080px\",\n            \"distance_from_center\": 33.83\n        },\n        {\n            \"time_stamp\": 385.08,\n            \"size\": \"1148px X 1040px\",\n            \"distance_from_center\": 353.61\n        },\n        {\n            \"time_stamp\": 394.76,\n            \"size\": \"1643px X 1034px\",\n            \"distance_from_center\": 137.03\n        },\n        {\n            \"time_stamp\": 395.73,\n            \"size\": \"1768px X 766px\",\n            \"distance_from_center\": 169.0\n        },\n        {\n            \"time_stamp\": 395.73,\n            \"size\": \"1781px X 1080px\",\n            \"distance_from_center\": 60.7\n        },\n        {\n            \"time_stamp\": 397.66,\n            \"size\": \"1807px X 738px\",\n            \"distance_from_center\": 177.17\n        },\n        {\n            \"time_stamp\": 397.66,\n            \"size\": \"1538px X 520px\",\n            \"distance_from_center\": 336.44\n        },\n        {\n            \"time_stamp\": 398.63,\n            \"size\": \"1794px X 634px\",\n            \"distance_from_center\": 227.82\n        },\n        {\n            \"time_stamp\": 398.63,\n            \"size\": \"1587px X 523px\",\n            \"distance_from_center\": 324.22\n        },\n        {\n            \"time_stamp\": 400.57,\n            \"size\": \"836px X 1080px\",\n            \"distance_from_center\": 541.85\n        },\n        {\n            \"time_stamp\": 401.53,\n            \"size\": \"1404px X 592px\",\n            \"distance_from_center\": 264.22\n        },\n        {\n            \"time_stamp\": 403.47,\n            \"size\": \"848px X 824px\",\n            \"distance_from_center\": 543.11\n        },\n        {\n            \"time_stamp\": 404.44,\n            \"size\": \"941px X 1011px\",\n            \"distance_from_center\": 488.8\n        },\n        {\n            \"time_stamp\": 404.44,\n            \"size\": \"974px X 283px\",\n            \"distance_from_center\": 616.62\n        },\n        {\n            \"time_stamp\": 405.4,\n            \"size\": \"909px X 1047px\",\n            \"distance_from_center\": 500.96\n        },\n        {\n            \"time_stamp\": 407.34,\n            \"size\": \"669px X 952px\",\n            \"distance_from_center\": 625.82\n        },\n        {\n            \"time_stamp\": 413.15,\n            \"size\": \"1765px X 841px\",\n            \"distance_from_center\": 137.24\n        },\n        {\n            \"time_stamp\": 417.02,\n            \"size\": \"1842px X 1078px\",\n            \"distance_from_center\": 31.32\n        },\n        {\n            \"time_stamp\": 417.98,\n            \"size\": \"1855px X 1080px\",\n            \"distance_from_center\": 29.16\n        },\n        {\n            \"time_stamp\": 431.53,\n            \"size\": \"1680px X 730px\",\n            \"distance_from_center\": 211.75\n        },\n        {\n            \"time_stamp\": 432.5,\n            \"size\": \"1218px X 725px\",\n            \"distance_from_center\": 382.54\n        },\n        {\n            \"time_stamp\": 433.47,\n            \"size\": \"1318px X 647px\",\n            \"distance_from_center\": 364.09\n        },\n        {\n            \"time_stamp\": 434.43,\n            \"size\": \"1529px X 727px\",\n            \"distance_from_center\": 261.64\n        },\n        {\n            \"time_stamp\": 434.43,\n            \"size\": \"489px X 735px\",\n            \"distance_from_center\": 733.93\n        },\n        {\n            \"time_stamp\": 442.18,\n            \"size\": \"1080px X 806px\",\n            \"distance_from_center\": 427.12\n        },\n        {\n            \"time_stamp\": 442.18,\n            \"size\": \"641px X 814px\",\n            \"distance_from_center\": 230.85\n        },\n        {\n            \"time_stamp\": 442.18,\n            \"size\": \"386px X 793px\",\n            \"distance_from_center\": 157.83\n        },\n        {\n            \"time_stamp\": 444.11,\n            \"size\": \"527px X 789px\",\n            \"distance_from_center\": 704.79\n        },\n        {\n            \"time_stamp\": 444.11,\n            \"size\": \"875px X 801px\",\n            \"distance_from_center\": 540.32\n        },\n        {\n            \"time_stamp\": 445.08,\n            \"size\": \"653px X 1080px\",\n            \"distance_from_center\": 633.3\n        },\n        {\n            \"time_stamp\": 445.08,\n            \"size\": \"1823px X 1078px\",\n            \"distance_from_center\": 36.02\n        },\n        {\n            \"time_stamp\": 446.05,\n            \"size\": \"1838px X 1080px\",\n            \"distance_from_center\": 28.05\n        },\n        {\n            \"time_stamp\": 447.01,\n            \"size\": \"1835px X 1080px\",\n            \"distance_from_center\": 28.74\n        },\n        {\n            \"time_stamp\": 449.92,\n            \"size\": \"1837px X 1080px\",\n            \"distance_from_center\": 37.12\n        },\n        {\n            \"time_stamp\": 452.82,\n            \"size\": \"1852px X 1080px\",\n            \"distance_from_center\": 33.77\n        },\n        {\n            \"time_stamp\": 458.62,\n            \"size\": \"324px X 300px\",\n            \"distance_from_center\": 887.81\n        },\n        {\n            \"time_stamp\": 460.56,\n            \"size\": \"437px X 300px\",\n            \"distance_from_center\": 836.98\n        },\n        {\n            \"time_stamp\": 461.53,\n            \"size\": \"354px X 313px\",\n            \"distance_from_center\": 871.36\n        },\n        {\n            \"time_stamp\": 469.27,\n            \"size\": \"1447px X 528px\",\n            \"distance_from_center\": 354.74\n        },\n        {\n            \"time_stamp\": 470.24,\n            \"size\": \"1735px X 527px\",\n            \"distance_from_center\": 291.39\n        },\n        {\n            \"time_stamp\": 472.17,\n            \"size\": \"850px X 503px\",\n            \"distance_from_center\": 607.67\n        },\n        {\n            \"time_stamp\": 473.14,\n            \"size\": \"888px X 510px\",\n            \"distance_from_center\": 586.03\n        },\n        {\n            \"time_stamp\": 474.11,\n            \"size\": \"902px X 506px\",\n            \"distance_from_center\": 583.9\n        },\n        {\n            \"time_stamp\": 474.11,\n            \"size\": \"549px X 545px\",\n            \"distance_from_center\": 734.53\n        },\n        {\n            \"time_stamp\": 475.07,\n            \"size\": \"946px X 505px\",\n            \"distance_from_center\": 564.27\n        },\n        {\n            \"time_stamp\": 476.04,\n            \"size\": \"841px X 506px\",\n            \"distance_from_center\": 609.17\n        },\n        {\n            \"time_stamp\": 477.98,\n            \"size\": \"1380px X 723px\",\n            \"distance_from_center\": 283.25\n        },\n        {\n            \"time_stamp\": 480.88,\n            \"size\": \"1698px X 524px\",\n            \"distance_from_center\": 297.7\n        },\n        {\n            \"time_stamp\": 483.78,\n            \"size\": \"1149px X 645px\",\n            \"distance_from_center\": 244.98\n        },\n        {\n            \"time_stamp\": 484.75,\n            \"size\": \"686px X 828px\",\n            \"distance_from_center\": 366.35\n        },\n        {\n            \"time_stamp\": 484.75,\n            \"size\": \"971px X 823px\",\n            \"distance_from_center\": 328.75\n        },\n        {\n            \"time_stamp\": 485.72,\n            \"size\": \"1763px X 830px\",\n            \"distance_from_center\": 143.87\n        },\n        {\n            \"time_stamp\": 486.69,\n            \"size\": \"888px X 705px\",\n            \"distance_from_center\": 453.26\n        },\n        {\n            \"time_stamp\": 487.65,\n            \"size\": \"1551px X 777px\",\n            \"distance_from_center\": 146.36\n        },\n        {\n            \"time_stamp\": 488.62,\n            \"size\": \"809px X 838px\",\n            \"distance_from_center\": 511.72\n        },\n        {\n            \"time_stamp\": 489.59,\n            \"size\": \"649px X 890px\",\n            \"distance_from_center\": 430.22\n        },\n        {\n            \"time_stamp\": 489.59,\n            \"size\": \"668px X 702px\",\n            \"distance_from_center\": 653.55\n        },\n        {\n            \"time_stamp\": 490.56,\n            \"size\": \"629px X 895px\",\n            \"distance_from_center\": 406.34\n        },\n        {\n            \"time_stamp\": 496.36,\n            \"size\": \"1769px X 833px\",\n            \"distance_from_center\": 138.28\n        },\n        {\n            \"time_stamp\": 497.33,\n            \"size\": \"1869px X 810px\",\n            \"distance_from_center\": 105.37\n        },\n        {\n            \"time_stamp\": 499.27,\n            \"size\": \"874px X 781px\",\n            \"distance_from_center\": 320.04\n        },\n        {\n            \"time_stamp\": 499.27,\n            \"size\": \"824px X 844px\",\n            \"distance_from_center\": 119.61\n        },\n        {\n            \"time_stamp\": 499.27,\n            \"size\": \"1101px X 635px\",\n            \"distance_from_center\": 359.04\n        },\n        {\n            \"time_stamp\": 500.23,\n            \"size\": \"1875px X 837px\",\n            \"distance_from_center\": 117.9\n        },\n        {\n            \"time_stamp\": 520.55,\n            \"size\": \"386px X 586px\",\n            \"distance_from_center\": 80.0\n        },\n        {\n            \"time_stamp\": 521.52,\n            \"size\": \"1055px X 913px\",\n            \"distance_from_center\": 440.32\n        },\n        {\n            \"time_stamp\": 521.52,\n            \"size\": \"701px X 934px\",\n            \"distance_from_center\": 284.9\n        },\n        {\n            \"time_stamp\": 521.52,\n            \"size\": \"587px X 869px\",\n            \"distance_from_center\": 202.74\n        },\n        {\n            \"time_stamp\": 521.52,\n            \"size\": \"944px X 900px\",\n            \"distance_from_center\": 465.48\n        },\n        {\n            \"time_stamp\": 521.52,\n            \"size\": \"668px X 874px\",\n            \"distance_from_center\": 630.42\n        },\n        {\n            \"time_stamp\": 522.49,\n            \"size\": \"690px X 716px\",\n            \"distance_from_center\": 611.51\n        },\n        {\n            \"time_stamp\": 522.49,\n            \"size\": \"444px X 907px\",\n            \"distance_from_center\": 109.97\n        },\n        {\n            \"time_stamp\": 523.46,\n            \"size\": \"820px X 910px\",\n            \"distance_from_center\": 554.89\n        },\n        {\n            \"time_stamp\": 523.46,\n            \"size\": \"1162px X 793px\",\n            \"distance_from_center\": 388.78\n        },\n        {\n            \"time_stamp\": 524.42,\n            \"size\": \"1036px X 566px\",\n            \"distance_from_center\": 447.72\n        },\n        {\n            \"time_stamp\": 524.42,\n            \"size\": \"1050px X 664px\",\n            \"distance_from_center\": 431.86\n        },\n        {\n            \"time_stamp\": 525.39,\n            \"size\": \"1753px X 638px\",\n            \"distance_from_center\": 102.5\n        },\n        {\n            \"time_stamp\": 525.39,\n            \"size\": \"1557px X 499px\",\n            \"distance_from_center\": 215.45\n        },\n        {\n            \"time_stamp\": 526.36,\n            \"size\": \"1746px X 801px\",\n            \"distance_from_center\": 89.43\n        },\n        {\n            \"time_stamp\": 527.33,\n            \"size\": \"735px X 226px\",\n            \"distance_from_center\": 722.97\n        },\n        {\n            \"time_stamp\": 527.33,\n            \"size\": \"914px X 493px\",\n            \"distance_from_center\": 157.1\n        },\n        {\n            \"time_stamp\": 528.29,\n            \"size\": \"691px X 216px\",\n            \"distance_from_center\": 740.86\n        },\n        {\n            \"time_stamp\": 528.29,\n            \"size\": \"998px X 494px\",\n            \"distance_from_center\": 183.15\n        },\n        {\n            \"time_stamp\": 529.26,\n            \"size\": \"713px X 216px\",\n            \"distance_from_center\": 730.65\n        },\n        {\n            \"time_stamp\": 530.23,\n            \"size\": \"682px X 216px\",\n            \"distance_from_center\": 746.7\n        },\n        {\n            \"time_stamp\": 530.23,\n            \"size\": \"1097px X 496px\",\n            \"distance_from_center\": 215.18\n        },\n        {\n            \"time_stamp\": 531.2,\n            \"size\": \"1088px X 1000px\",\n            \"distance_from_center\": 405.3\n        },\n        {\n            \"time_stamp\": 531.2,\n            \"size\": \"1368px X 751px\",\n            \"distance_from_center\": 294.81\n        },\n        {\n            \"time_stamp\": 532.16,\n            \"size\": \"1342px X 1002px\",\n            \"distance_from_center\": 278.87\n        },\n        {\n            \"time_stamp\": 533.13,\n            \"size\": \"1589px X 830px\",\n            \"distance_from_center\": 196.85\n        },\n        {\n            \"time_stamp\": 533.13,\n            \"size\": \"1216px X 1062px\",\n            \"distance_from_center\": 342.79\n        },\n        {\n            \"time_stamp\": 534.1,\n            \"size\": \"1554px X 807px\",\n            \"distance_from_center\": 215.8\n        },\n        {\n            \"time_stamp\": 536.04,\n            \"size\": \"1508px X 728px\",\n            \"distance_from_center\": 257.55\n        },\n        {\n            \"time_stamp\": 537.0,\n            \"size\": \"1424px X 633px\",\n            \"distance_from_center\": 327.14\n        },\n        {\n            \"time_stamp\": 537.0,\n            \"size\": \"871px X 1021px\",\n            \"distance_from_center\": 522.33\n        },\n        {\n            \"time_stamp\": 537.97,\n            \"size\": \"1836px X 1045px\",\n            \"distance_from_center\": 39.53\n        },\n        {\n            \"time_stamp\": 538.94,\n            \"size\": \"1763px X 1037px\",\n            \"distance_from_center\": 81.33\n        },\n        {\n            \"time_stamp\": 539.91,\n            \"size\": \"1868px X 1054px\",\n            \"distance_from_center\": 28.55\n        },\n        {\n            \"time_stamp\": 540.87,\n            \"size\": \"1819px X 1025px\",\n            \"distance_from_center\": 46.78\n        },\n        {\n            \"time_stamp\": 541.84,\n            \"size\": \"1864px X 1041px\",\n            \"distance_from_center\": 34.09\n        },\n        {\n            \"time_stamp\": 543.78,\n            \"size\": \"1483px X 1071px\",\n            \"distance_from_center\": 218.3\n        },\n        {\n            \"time_stamp\": 544.74,\n            \"size\": \"1625px X 1055px\",\n            \"distance_from_center\": 147.53\n        },\n        {\n            \"time_stamp\": 545.71,\n            \"size\": \"1605px X 1056px\",\n            \"distance_from_center\": 150.5\n        },\n        {\n            \"time_stamp\": 546.68,\n            \"size\": \"1096px X 1049px\",\n            \"distance_from_center\": 364.9\n        },\n        {\n            \"time_stamp\": 547.65,\n            \"size\": \"849px X 1056px\",\n            \"distance_from_center\": 535.36\n        },\n        {\n            \"time_stamp\": 548.61,\n            \"size\": \"776px X 1068px\",\n            \"distance_from_center\": 570.43\n        },\n        {\n            \"time_stamp\": 549.58,\n            \"size\": \"1245px X 882px\",\n            \"distance_from_center\": 324.21\n        },\n        {\n            \"time_stamp\": 553.45,\n            \"size\": \"1281px X 608px\",\n            \"distance_from_center\": 382.1\n        },\n        {\n            \"time_stamp\": 555.39,\n            \"size\": \"1443px X 602px\",\n            \"distance_from_center\": 316.38\n        },\n        {\n            \"time_stamp\": 556.36,\n            \"size\": \"826px X 781px\",\n            \"distance_from_center\": 279.75\n        },\n        {\n            \"time_stamp\": 557.32,\n            \"size\": \"1254px X 724px\",\n            \"distance_from_center\": 345.92\n        },\n        {\n            \"time_stamp\": 559.26,\n            \"size\": \"795px X 771px\",\n            \"distance_from_center\": 582.98\n        },\n        {\n            \"time_stamp\": 559.26,\n            \"size\": \"497px X 755px\",\n            \"distance_from_center\": 729.76\n        },\n        {\n            \"time_stamp\": 560.23,\n            \"size\": \"641px X 746px\",\n            \"distance_from_center\": 660.73\n        },\n        {\n            \"time_stamp\": 560.23,\n            \"size\": \"489px X 756px\",\n            \"distance_from_center\": 733.18\n        },\n        {\n            \"time_stamp\": 561.19,\n            \"size\": \"1076px X 703px\",\n            \"distance_from_center\": 414.71\n        },\n        {\n            \"time_stamp\": 561.19,\n            \"size\": \"639px X 637px\",\n            \"distance_from_center\": 285.83\n        },\n        {\n            \"time_stamp\": 561.19,\n            \"size\": \"469px X 740px\",\n            \"distance_from_center\": 744.75\n        },\n        {\n            \"time_stamp\": 561.19,\n            \"size\": \"884px X 824px\",\n            \"distance_from_center\": 533.28\n        },\n        {\n            \"time_stamp\": 562.16,\n            \"size\": \"744px X 782px\",\n            \"distance_from_center\": 606.32\n        },\n        {\n            \"time_stamp\": 562.16,\n            \"size\": \"324px X 644px\",\n            \"distance_from_center\": 184.5\n        },\n        {\n            \"time_stamp\": 563.13,\n            \"size\": \"818px X 792px\",\n            \"distance_from_center\": 569.28\n        },\n        {\n            \"time_stamp\": 563.13,\n            \"size\": \"340px X 672px\",\n            \"distance_from_center\": 184.19\n        },\n        {\n            \"time_stamp\": 564.1,\n            \"size\": \"819px X 819px\",\n            \"distance_from_center\": 562.48\n        },\n        {\n            \"time_stamp\": 564.1,\n            \"size\": \"366px X 467px\",\n            \"distance_from_center\": 172.51\n        },\n        {\n            \"time_stamp\": 564.1,\n            \"size\": \"645px X 768px\",\n            \"distance_from_center\": 640.08\n        },\n        {\n            \"time_stamp\": 565.06,\n            \"size\": \"761px X 408px\",\n            \"distance_from_center\": 579.11\n        },\n        {\n            \"time_stamp\": 565.06,\n            \"size\": \"765px X 614px\",\n            \"distance_from_center\": 575.55\n        },\n        {\n            \"time_stamp\": 565.06,\n            \"size\": \"354px X 486px\",\n            \"distance_from_center\": 130.51\n        },\n        {\n            \"time_stamp\": 566.03,\n            \"size\": \"700px X 800px\",\n            \"distance_from_center\": 615.1\n        },\n        {\n            \"time_stamp\": 566.03,\n            \"size\": \"357px X 410px\",\n            \"distance_from_center\": 121.65\n        },\n        {\n            \"time_stamp\": 567.0,\n            \"size\": \"648px X 832px\",\n            \"distance_from_center\": 640.84\n        },\n        {\n            \"time_stamp\": 567.0,\n            \"size\": \"737px X 363px\",\n            \"distance_from_center\": 593.52\n        },\n        {\n            \"time_stamp\": 567.0,\n            \"size\": \"741px X 545px\",\n            \"distance_from_center\": 586.93\n        },\n        {\n            \"time_stamp\": 567.0,\n            \"size\": \"354px X 420px\",\n            \"distance_from_center\": 114.01\n        },\n        {\n            \"time_stamp\": 567.97,\n            \"size\": \"590px X 785px\",\n            \"distance_from_center\": 672.53\n        },\n        {\n            \"time_stamp\": 567.97,\n            \"size\": \"340px X 380px\",\n            \"distance_from_center\": 130.46\n        },\n        {\n            \"time_stamp\": 567.97,\n            \"size\": \"795px X 370px\",\n            \"distance_from_center\": 564.86\n        },\n        {\n            \"time_stamp\": 568.94,\n            \"size\": \"657px X 806px\",\n            \"distance_from_center\": 638.72\n        },\n        {\n            \"time_stamp\": 568.94,\n            \"size\": \"720px X 379px\",\n            \"distance_from_center\": 599.09\n        },\n        {\n            \"time_stamp\": 568.94,\n            \"size\": \"718px X 559px\",\n            \"distance_from_center\": 599.18\n        },\n        {\n            \"time_stamp\": 568.94,\n            \"size\": \"344px X 363px\",\n            \"distance_from_center\": 136.32\n        },\n        {\n            \"time_stamp\": 576.68,\n            \"size\": \"339px X 701px\",\n            \"distance_from_center\": 35.73\n        },\n        {\n            \"time_stamp\": 577.64,\n            \"size\": \"306px X 702px\",\n            \"distance_from_center\": 20.35\n        },\n        {\n            \"time_stamp\": 578.61,\n            \"size\": \"447px X 822px\",\n            \"distance_from_center\": 6.51\n        },\n        {\n            \"time_stamp\": 579.58,\n            \"size\": \"442px X 904px\",\n            \"distance_from_center\": 108.11\n        },\n        {\n            \"time_stamp\": 580.55,\n            \"size\": \"407px X 688px\",\n            \"distance_from_center\": 73.33\n        },\n        {\n            \"time_stamp\": 581.51,\n            \"size\": \"457px X 815px\",\n            \"distance_from_center\": 91.37\n        },\n        {\n            \"time_stamp\": 582.48,\n            \"size\": \"426px X 537px\",\n            \"distance_from_center\": 210.24\n        },\n        {\n            \"time_stamp\": 588.29,\n            \"size\": \"1226px X 896px\",\n            \"distance_from_center\": 316.55\n        },\n        {\n            \"time_stamp\": 589.26,\n            \"size\": \"939px X 827px\",\n            \"distance_from_center\": 121.95\n        },\n        {\n            \"time_stamp\": 590.22,\n            \"size\": \"421px X 412px\",\n            \"distance_from_center\": 633.04\n        },\n        {\n            \"time_stamp\": 590.22,\n            \"size\": \"707px X 751px\",\n            \"distance_from_center\": 179.24\n        },\n        {\n            \"time_stamp\": 591.19,\n            \"size\": \"296px X 403px\",\n            \"distance_from_center\": 879.59\n        },\n        {\n            \"time_stamp\": 592.16,\n            \"size\": \"847px X 516px\",\n            \"distance_from_center\": 174.27\n        },\n        {\n            \"time_stamp\": 593.13,\n            \"size\": \"811px X 860px\",\n            \"distance_from_center\": 179.99\n        },\n        {\n            \"time_stamp\": 593.13,\n            \"size\": \"818px X 572px\",\n            \"distance_from_center\": 150.12\n        },\n        {\n            \"time_stamp\": 594.09,\n            \"size\": \"797px X 870px\",\n            \"distance_from_center\": 167.0\n        },\n        {\n            \"time_stamp\": 596.03,\n            \"size\": \"625px X 726px\",\n            \"distance_from_center\": 133.79\n        },\n        {\n            \"time_stamp\": 596.03,\n            \"size\": \"449px X 714px\",\n            \"distance_from_center\": 83.3\n        },\n        {\n            \"time_stamp\": 597.0,\n            \"size\": \"328px X 470px\",\n            \"distance_from_center\": 177.2\n        },\n        {\n            \"time_stamp\": 597.96,\n            \"size\": \"398px X 319px\",\n            \"distance_from_center\": 278.95\n        },\n        {\n            \"time_stamp\": 600.87,\n            \"size\": \"1030px X 596px\",\n            \"distance_from_center\": 250.36\n        },\n        {\n            \"time_stamp\": 601.83,\n            \"size\": \"538px X 814px\",\n            \"distance_from_center\": 302.22\n        },\n        {\n            \"time_stamp\": 621.19,\n            \"size\": \"1024px X 252px\",\n            \"distance_from_center\": 609.8\n        },\n        {\n            \"time_stamp\": 623.12,\n            \"size\": \"1007px X 665px\",\n            \"distance_from_center\": 241.55\n        },\n        {\n            \"time_stamp\": 624.09,\n            \"size\": \"1173px X 581px\",\n            \"distance_from_center\": 281.83\n        },\n        {\n            \"time_stamp\": 625.06,\n            \"size\": \"1087px X 534px\",\n            \"distance_from_center\": 281.45\n        },\n        {\n            \"time_stamp\": 626.03,\n            \"size\": \"1043px X 568px\",\n            \"distance_from_center\": 260.11\n        },\n        {\n            \"time_stamp\": 626.99,\n            \"size\": \"266px X 1011px\",\n            \"distance_from_center\": 827.25\n        },\n        {\n            \"time_stamp\": 627.96,\n            \"size\": \"635px X 567px\",\n            \"distance_from_center\": 279.29\n        },\n        {\n            \"time_stamp\": 634.73,\n            \"size\": \"621px X 890px\",\n            \"distance_from_center\": 94.96\n        },\n        {\n            \"time_stamp\": 634.73,\n            \"size\": \"1041px X 897px\",\n            \"distance_from_center\": 198.51\n        },\n        {\n            \"time_stamp\": 635.7,\n            \"size\": \"411px X 732px\",\n            \"distance_from_center\": 85.27\n        },\n        {\n            \"time_stamp\": 636.67,\n            \"size\": \"459px X 782px\",\n            \"distance_from_center\": 115.38\n        },\n        {\n            \"time_stamp\": 636.67,\n            \"size\": \"1009px X 771px\",\n            \"distance_from_center\": 352.29\n        },\n        {\n            \"time_stamp\": 637.64,\n            \"size\": \"462px X 820px\",\n            \"distance_from_center\": 56.12\n        },\n        {\n            \"time_stamp\": 637.64,\n            \"size\": \"760px X 668px\",\n            \"distance_from_center\": 178.2\n        },\n        {\n            \"time_stamp\": 638.6,\n            \"size\": \"351px X 584px\",\n            \"distance_from_center\": 261.27\n        },\n        {\n            \"time_stamp\": 638.6,\n            \"size\": \"353px X 588px\",\n            \"distance_from_center\": 241.29\n        },\n        {\n            \"time_stamp\": 639.57,\n            \"size\": \"354px X 601px\",\n            \"distance_from_center\": 266.5\n        },\n        {\n            \"time_stamp\": 639.57,\n            \"size\": \"409px X 725px\",\n            \"distance_from_center\": 205.45\n        },\n        {\n            \"time_stamp\": 640.54,\n            \"size\": \"356px X 605px\",\n            \"distance_from_center\": 269.43\n        },\n        {\n            \"time_stamp\": 640.54,\n            \"size\": \"360px X 648px\",\n            \"distance_from_center\": 169.15\n        },\n        {\n            \"time_stamp\": 641.51,\n            \"size\": \"366px X 613px\",\n            \"distance_from_center\": 279.2\n        },\n        {\n            \"time_stamp\": 641.51,\n            \"size\": \"355px X 628px\",\n            \"distance_from_center\": 138.08\n        },\n        {\n            \"time_stamp\": 642.48,\n            \"size\": \"862px X 876px\",\n            \"distance_from_center\": 533.07\n        },\n        {\n            \"time_stamp\": 642.48,\n            \"size\": \"817px X 620px\",\n            \"distance_from_center\": 512.78\n        },\n        {\n            \"time_stamp\": 642.48,\n            \"size\": \"632px X 761px\",\n            \"distance_from_center\": 425.1\n        },\n        {\n            \"time_stamp\": 642.48,\n            \"size\": \"580px X 863px\",\n            \"distance_from_center\": 364.24\n        },\n        {\n            \"time_stamp\": 643.44,\n            \"size\": \"407px X 677px\",\n            \"distance_from_center\": 281.1\n        },\n        {\n            \"time_stamp\": 643.44,\n            \"size\": \"382px X 619px\",\n            \"distance_from_center\": 273.67\n        },\n        {\n            \"time_stamp\": 643.44,\n            \"size\": \"204px X 377px\",\n            \"distance_from_center\": 512.5\n        },\n        {\n            \"time_stamp\": 644.41,\n            \"size\": \"407px X 670px\",\n            \"distance_from_center\": 279.77\n        },\n        {\n            \"time_stamp\": 644.41,\n            \"size\": \"384px X 623px\",\n            \"distance_from_center\": 270.88\n        },\n        {\n            \"time_stamp\": 644.41,\n            \"size\": \"204px X 376px\",\n            \"distance_from_center\": 511.46\n        },\n        {\n            \"time_stamp\": 645.38,\n            \"size\": \"342px X 533px\",\n            \"distance_from_center\": 239.42\n        },\n        {\n            \"time_stamp\": 645.38,\n            \"size\": \"340px X 563px\",\n            \"distance_from_center\": 258.85\n        },\n        {\n            \"time_stamp\": 646.35,\n            \"size\": \"343px X 578px\",\n            \"distance_from_center\": 264.17\n        },\n        {\n            \"time_stamp\": 647.31,\n            \"size\": \"353px X 589px\",\n            \"distance_from_center\": 273.85\n        },\n        {\n            \"time_stamp\": 647.31,\n            \"size\": \"360px X 601px\",\n            \"distance_from_center\": 224.21\n        },\n        {\n            \"time_stamp\": 648.28,\n            \"size\": \"367px X 604px\",\n            \"distance_from_center\": 287.86\n        },\n        {\n            \"time_stamp\": 649.25,\n            \"size\": \"369px X 633px\",\n            \"distance_from_center\": 287.78\n        },\n        {\n            \"time_stamp\": 650.22,\n            \"size\": \"651px X 345px\",\n            \"distance_from_center\": 369.14\n        },\n        {\n            \"time_stamp\": 650.22,\n            \"size\": \"377px X 652px\",\n            \"distance_from_center\": 294.06\n        },\n        {\n            \"time_stamp\": 651.18,\n            \"size\": \"383px X 646px\",\n            \"distance_from_center\": 301.61\n        },\n        {\n            \"time_stamp\": 651.18,\n            \"size\": \"628px X 355px\",\n            \"distance_from_center\": 362.98\n        },\n        {\n            \"time_stamp\": 652.15,\n            \"size\": \"388px X 680px\",\n            \"distance_from_center\": 303.98\n        },\n        {\n            \"time_stamp\": 653.12,\n            \"size\": \"390px X 657px\",\n            \"distance_from_center\": 310.31\n        },\n        {\n            \"time_stamp\": 654.09,\n            \"size\": \"392px X 671px\",\n            \"distance_from_center\": 313.4\n        },\n        {\n            \"time_stamp\": 655.05,\n            \"size\": \"391px X 660px\",\n            \"distance_from_center\": 195.2\n        },\n        {\n            \"time_stamp\": 655.05,\n            \"size\": \"403px X 686px\",\n            \"distance_from_center\": 312.62\n        },\n        {\n            \"time_stamp\": 656.02,\n            \"size\": \"413px X 693px\",\n            \"distance_from_center\": 313.93\n        },\n        {\n            \"time_stamp\": 656.02,\n            \"size\": \"380px X 717px\",\n            \"distance_from_center\": 220.17\n        },\n        {\n            \"time_stamp\": 656.99,\n            \"size\": \"415px X 706px\",\n            \"distance_from_center\": 317.36\n        },\n        {\n            \"time_stamp\": 656.99,\n            \"size\": \"380px X 691px\",\n            \"distance_from_center\": 221.17\n        },\n        {\n            \"time_stamp\": 657.96,\n            \"size\": \"409px X 714px\",\n            \"distance_from_center\": 266.64\n        },\n        {\n            \"time_stamp\": 657.96,\n            \"size\": \"456px X 712px\",\n            \"distance_from_center\": 348.15\n        },\n        {\n            \"time_stamp\": 657.96,\n            \"size\": \"596px X 708px\",\n            \"distance_from_center\": 316.87\n        },\n        {\n            \"time_stamp\": 658.92,\n            \"size\": \"487px X 739px\",\n            \"distance_from_center\": 326.69\n        },\n        {\n            \"time_stamp\": 658.92,\n            \"size\": \"440px X 722px\",\n            \"distance_from_center\": 261.0\n        },\n        {\n            \"time_stamp\": 658.92,\n            \"size\": \"829px X 718px\",\n            \"distance_from_center\": 508.54\n        },\n        {\n            \"time_stamp\": 662.8,\n            \"size\": \"399px X 687px\",\n            \"distance_from_center\": 281.51\n        },\n        {\n            \"time_stamp\": 662.8,\n            \"size\": \"436px X 781px\",\n            \"distance_from_center\": 332.6\n        },\n        {\n            \"time_stamp\": 663.76,\n            \"size\": \"499px X 672px\",\n            \"distance_from_center\": 296.99\n        },\n        {\n            \"time_stamp\": 663.76,\n            \"size\": \"861px X 597px\",\n            \"distance_from_center\": 533.35\n        },\n        {\n            \"time_stamp\": 663.76,\n            \"size\": \"558px X 639px\",\n            \"distance_from_center\": 401.44\n        },\n        {\n            \"time_stamp\": 664.73,\n            \"size\": \"422px X 742px\",\n            \"distance_from_center\": 278.14\n        },\n        {\n            \"time_stamp\": 664.73,\n            \"size\": \"421px X 652px\",\n            \"distance_from_center\": 370.11\n        },\n        {\n            \"time_stamp\": 664.73,\n            \"size\": \"619px X 740px\",\n            \"distance_from_center\": 396.3\n        },\n        {\n            \"time_stamp\": 665.7,\n            \"size\": \"450px X 815px\",\n            \"distance_from_center\": 251.91\n        },\n        {\n            \"time_stamp\": 665.7,\n            \"size\": \"486px X 877px\",\n            \"distance_from_center\": 404.27\n        }\n    ],\n    \"Coca-Cola\": [\n        {\n            \"time_stamp\": 2.87,\n            \"size\": \"827px X 604px\",\n            \"distance_from_center\": 584.06\n        },\n        {\n            \"time_stamp\": 4.8,\n            \"size\": \"958px X 900px\",\n            \"distance_from_center\": 488.91\n        },\n        {\n            \"time_stamp\": 10.61,\n            \"size\": \"803px X 782px\",\n            \"distance_from_center\": 549.39\n        },\n        {\n            \"time_stamp\": 49.32,\n            \"size\": \"670px X 756px\",\n            \"distance_from_center\": 629.05\n        },\n        {\n            \"time_stamp\": 232.2,\n            \"size\": \"863px X 688px\",\n            \"distance_from_center\": 177.07\n        },\n        {\n            \"time_stamp\": 233.17,\n            \"size\": \"643px X 729px\",\n            \"distance_from_center\": 238.84\n        },\n        {\n            \"time_stamp\": 237.04,\n            \"size\": \"568px X 659px\",\n            \"distance_from_center\": 210.31\n        },\n        {\n            \"time_stamp\": 237.04,\n            \"size\": \"1084px X 684px\",\n            \"distance_from_center\": 330.45\n        },\n        {\n            \"time_stamp\": 238.0,\n            \"size\": \"1229px X 646px\",\n            \"distance_from_center\": 400.13\n        },\n        {\n            \"time_stamp\": 238.0,\n            \"size\": \"574px X 651px\",\n            \"distance_from_center\": 214.42\n        },\n        {\n            \"time_stamp\": 238.0,\n            \"size\": \"562px X 1041px\",\n            \"distance_from_center\": 19.56\n        },\n        {\n            \"time_stamp\": 238.97,\n            \"size\": \"566px X 1017px\",\n            \"distance_from_center\": 31.04\n        },\n        {\n            \"time_stamp\": 238.97,\n            \"size\": \"570px X 695px\",\n            \"distance_from_center\": 192.11\n        },\n        {\n            \"time_stamp\": 239.94,\n            \"size\": \"557px X 1020px\",\n            \"distance_from_center\": 29.93\n        },\n        {\n            \"time_stamp\": 239.94,\n            \"size\": \"565px X 682px\",\n            \"distance_from_center\": 198.95\n        },\n        {\n            \"time_stamp\": 240.91,\n            \"size\": \"564px X 1018px\",\n            \"distance_from_center\": 30.88\n        },\n        {\n            \"time_stamp\": 240.91,\n            \"size\": \"566px X 663px\",\n            \"distance_from_center\": 208.4\n        },\n        {\n            \"time_stamp\": 241.87,\n            \"size\": \"564px X 1014px\",\n            \"distance_from_center\": 32.81\n        },\n        {\n            \"time_stamp\": 241.87,\n            \"size\": \"562px X 672px\",\n            \"distance_from_center\": 203.79\n        },\n        {\n            \"time_stamp\": 241.87,\n            \"size\": \"760px X 815px\",\n            \"distance_from_center\": 167.2\n        },\n        {\n            \"time_stamp\": 242.84,\n            \"size\": \"575px X 661px\",\n            \"distance_from_center\": 209.1\n        },\n        {\n            \"time_stamp\": 243.81,\n            \"size\": \"570px X 1020px\",\n            \"distance_from_center\": 29.74\n        },\n        {\n            \"time_stamp\": 243.81,\n            \"size\": \"1099px X 876px\",\n            \"distance_from_center\": 287.38\n        },\n        {\n            \"time_stamp\": 244.78,\n            \"size\": \"568px X 1016px\",\n            \"distance_from_center\": 31.92\n        },\n        {\n            \"time_stamp\": 260.26,\n            \"size\": \"907px X 759px\",\n            \"distance_from_center\": 530.48\n        },\n        {\n            \"time_stamp\": 264.13,\n            \"size\": \"560px X 675px\",\n            \"distance_from_center\": 186.22\n        },\n        {\n            \"time_stamp\": 266.07,\n            \"size\": \"946px X 739px\",\n            \"distance_from_center\": 508.3\n        },\n        {\n            \"time_stamp\": 355.09,\n            \"size\": \"443px X 813px\",\n            \"distance_from_center\": 141.05\n        },\n        {\n            \"time_stamp\": 357.02,\n            \"size\": \"435px X 839px\",\n            \"distance_from_center\": 127.28\n        },\n        {\n            \"time_stamp\": 493.46,\n            \"size\": \"641px X 749px\",\n            \"distance_from_center\": 112.41\n        },\n        {\n            \"time_stamp\": 494.43,\n            \"size\": \"655px X 773px\",\n            \"distance_from_center\": 82.78\n        },\n        {\n            \"time_stamp\": 632.8,\n            \"size\": \"853px X 709px\",\n            \"distance_from_center\": 190.79\n        }\n    ]\n}\n","output_type":"stream"}]},{"cell_type":"code","source":"# print(timestamps_json_object)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.463584Z","iopub.execute_input":"2024-07-08T11:04:33.463906Z","iopub.status.idle":"2024-07-08T11:04:33.470581Z","shell.execute_reply.started":"2024-07-08T11:04:33.463871Z","shell.execute_reply":"2024-07-08T11:04:33.469679Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# CLASS_IDS = {\n#     \"CocaCola\": 0,\n#     \"Pepsi\": 1\n# }","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.471688Z","iopub.execute_input":"2024-07-08T11:04:33.471990Z","iopub.status.idle":"2024-07-08T11:04:33.480806Z","shell.execute_reply.started":"2024-07-08T11:04:33.471966Z","shell.execute_reply":"2024-07-08T11:04:33.480016Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# video_path = \"/kaggle/input/video-pepsi-cola/videoplayback.mp4\"","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.481823Z","iopub.execute_input":"2024-07-08T11:04:33.482117Z","iopub.status.idle":"2024-07-08T11:04:33.494824Z","shell.execute_reply.started":"2024-07-08T11:04:33.482094Z","shell.execute_reply":"2024-07-08T11:04:33.493910Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def count_frames(video_path):\n#     container = av.open(video_path)\n#     total_frames = 0\n    \n#     for frame in container.decode(video=0):\n#         total_frames += 1\n    \n#     return total_frames","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.496169Z","iopub.execute_input":"2024-07-08T11:04:33.496769Z","iopub.status.idle":"2024-07-08T11:04:33.504972Z","shell.execute_reply.started":"2024-07-08T11:04:33.496743Z","shell.execute_reply":"2024-07-08T11:04:33.504098Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# container = av.open(video_path)\n# total_frames = count_frames(video_path)\n# total_frames","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.506193Z","iopub.execute_input":"2024-07-08T11:04:33.506822Z","iopub.status.idle":"2024-07-08T11:04:33.514163Z","shell.execute_reply.started":"2024-07-08T11:04:33.506789Z","shell.execute_reply":"2024-07-08T11:04:33.513403Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# container = av.open(video_path)\n# total_frames = sum(1 for _ in container.decode(video=0))\n# print(f\"Total number of frames in video: {total_frames}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.519719Z","iopub.execute_input":"2024-07-08T11:04:33.520093Z","iopub.status.idle":"2024-07-08T11:04:33.524040Z","shell.execute_reply.started":"2024-07-08T11:04:33.520069Z","shell.execute_reply":"2024-07-08T11:04:33.523154Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# import cv2\n# import numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.525201Z","iopub.execute_input":"2024-07-08T11:04:33.525531Z","iopub.status.idle":"2024-07-08T11:04:33.533902Z","shell.execute_reply.started":"2024-07-08T11:04:33.525501Z","shell.execute_reply":"2024-07-08T11:04:33.533045Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# detection_results, total_frames = process_video(video_path, frame_skip=5)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.535043Z","iopub.execute_input":"2024-07-08T11:04:33.535349Z","iopub.status.idle":"2024-07-08T11:04:33.543060Z","shell.execute_reply.started":"2024-07-08T11:04:33.535324Z","shell.execute_reply":"2024-07-08T11:04:33.542320Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# detection_results","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.544124Z","iopub.execute_input":"2024-07-08T11:04:33.544437Z","iopub.status.idle":"2024-07-08T11:04:33.552555Z","shell.execute_reply.started":"2024-07-08T11:04:33.544413Z","shell.execute_reply":"2024-07-08T11:04:33.551683Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# total_frames","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.553674Z","iopub.execute_input":"2024-07-08T11:04:33.554078Z","iopub.status.idle":"2024-07-08T11:04:33.561998Z","shell.execute_reply.started":"2024-07-08T11:04:33.554053Z","shell.execute_reply":"2024-07-08T11:04:33.561175Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# img = cv2.imread('/kaggle/input/colaco/cola.png')\n# img_cv2 = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n        \n#         # Perform object detection on the frame\n# results = model(img_cv2)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.563161Z","iopub.execute_input":"2024-07-08T11:04:33.564085Z","iopub.status.idle":"2024-07-08T11:04:33.571272Z","shell.execute_reply.started":"2024-07-08T11:04:33.564060Z","shell.execute_reply":"2024-07-08T11:04:33.570571Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.572285Z","iopub.execute_input":"2024-07-08T11:04:33.572601Z","iopub.status.idle":"2024-07-08T11:04:33.580480Z","shell.execute_reply.started":"2024-07-08T11:04:33.572578Z","shell.execute_reply":"2024-07-08T11:04:33.579640Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# det = model.predict('/kaggle/input/colaco/cola.png')","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.581628Z","iopub.execute_input":"2024-07-08T11:04:33.582297Z","iopub.status.idle":"2024-07-08T11:04:33.591337Z","shell.execute_reply.started":"2024-07-08T11:04:33.582266Z","shell.execute_reply":"2024-07-08T11:04:33.590519Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# det[0].boxes.id","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.592326Z","iopub.execute_input":"2024-07-08T11:04:33.592659Z","iopub.status.idle":"2024-07-08T11:04:33.600721Z","shell.execute_reply.started":"2024-07-08T11:04:33.592627Z","shell.execute_reply":"2024-07-08T11:04:33.599756Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# detections","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:04:33.601828Z","iopub.execute_input":"2024-07-08T11:04:33.602171Z","iopub.status.idle":"2024-07-08T11:04:33.611427Z","shell.execute_reply.started":"2024-07-08T11:04:33.602140Z","shell.execute_reply":"2024-07-08T11:04:33.610537Z"},"trusted":true},"execution_count":37,"outputs":[]}]}